{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a6fdaec-a86e-47f7-9ed9-3d0242e5a9b3",
   "metadata": {},
   "source": [
    "# ðŸ“Š Evaluation in Flotorch\n",
    "\n",
    "[Flotorch](https://www.flotorch.ai/) provides a comprehensive evaluation framework for Retrieval-Augmented Generation (RAG) systems. It helps assess and compare Large Language Models (LLMs) based on relevance, quality, cost, and performance to support enterprise-grade deployments.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Key Evaluation Features\n",
    "\n",
    "- **Automated LLM Evaluation**  \n",
    "  Flotorch automates evaluation across:\n",
    "  - Relevance\n",
    "  - Fluency\n",
    "  - Robustness\n",
    "  - Cost\n",
    "  - Execution Speed\n",
    "\n",
    "- **Performance Metrics**  \n",
    "  It generates quantitative scores for evaluating how well a model performs across different criteria.\n",
    "\n",
    "- **Cost and Time Insights**  \n",
    "  Offers pricing and latency breakdowns for different LLM setups, enabling cost-effective choices.\n",
    "\n",
    "- **Data-Driven Decision-Making**  \n",
    "  Helps teams align LLM usage with specific application goals, budget, and performance needs.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Evaluation Workflow\n",
    "\n",
    "1. **Experiment Configuration**  \n",
    "   Define models, parameters, and goals for evaluation.\n",
    "\n",
    "2. **Automated Execution**  \n",
    "   Run evaluation pipelines to generate performance data.\n",
    "\n",
    "3. **Results Analysis**  \n",
    "   View dashboards or reports that summarize evaluation results.\n",
    "\n",
    "4. **Expert Evaluation (Optional)**  \n",
    "   Combine automatic evaluation with human review for more nuanced feedback.\n",
    "\n",
    "---\n",
    "\n",
    "This evaluation framework enables continuous monitoring, benchmarking, and optimization of RAG systems using LLMs, helping organizations deploy more reliable and efficient AI solutions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e067c92-f0f4-4135-a351-bf1f7c10d56d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/eval_prompt.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      2\u001b[0m prompt_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/eval_prompt.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/eval_prompt.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "prompt_file_path = './data/eval_prompt.json'\n",
    "with open(prompt_file_path, 'r') as f:\n",
    "    prompt = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce74afb-8660-4576-b446-2b73f4b38191",
   "metadata": {},
   "source": [
    "## Load experiment config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7bb7517-ea81-426b-8d2f-2e246970b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config_data = {\n",
    "            \"temp_retrieval_llm\": \"0.1\",\n",
    "            \"retrival_service\": \"bedrock\",\n",
    "            \"eval_retrieval_model\": \"bedrock/cohere.command-r-v1:0\",\n",
    "            \"eval_prompt\": prompt\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54042c51-bdab-4524-922c-d65651d09004",
   "metadata": {},
   "source": [
    "## Load inference metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "982fb77b-f992-4cf9-ba5f-6e8e63941a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f\"./results/{exp_config_data['retrival_service']}_inference_metrics.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59153913-5f0a-4a5f-8fd6-a7af3f5ae19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a0c847c-dec5-4bb0-a29f-1f74f2ee5d91",
   "metadata": {},
   "source": [
    "## Load Evaluator Class\n",
    "\n",
    "### ðŸ§  Evaluation with `CustomEvaluator`\n",
    "\n",
    "```python\n",
    "processor = CustomEvaluator(evaluator_llm=exp_config_data['eval_retrieval_model'])\n",
    "results = processor.evaluate(data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Step-by-Step Breakdown\n",
    "\n",
    "| Line | Description |\n",
    "|------|-------------|\n",
    "| `processor = CustomEvaluator(...)` | Instantiates a `CustomEvaluator` using a language model specified in the config (`exp_config_data['eval_retrieval_model']`). |\n",
    "| `results = processor.evaluate(data)` | Runs the evaluation on the `data` using the evaluator, returning performance metrics or scoring output. |\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ§© Key Components\n",
    "\n",
    "- **`CustomEvaluator`**: A custom class designed to handle evaluation logic, potentially wrapping RAGAS or similar frameworks.\n",
    "- **`evaluator_llm`**: The evaluation language model (e.g. GPT, Claude, etc.) used for scoring responses.\n",
    "- **`data`**: A list of evaluation items (e.g. questions, answers, reference contexts).\n",
    "- **`results`**: The output from the evaluation â€” typically a dictionary or structured result with metric scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "913229fb-86b2-4da2-9321-a461618a5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluator import CustomEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d81ee62-cff0-4581-9274-ba047fe199b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'query_metadata', 'reference_contexts', 'gt_answer', 'test'])\n",
      "Assume you are a human expert in grading Python function implementations. You are given a function definition, a ground truth implementation, a model prediction, and example test cases provided in the test field. Judge if the model's implementation matches the ground truth by following these steps:\n",
      "\n",
      "1. Assume the Ground Truth is always correct.\n",
      "2. If the Prediction is incomplete or shows it does not attempt a real solution, set \"score\" to 0.\n",
      "3. If the Prediction exactly matches the Ground Truth implementation, set \"score\" to 1.\n",
      "4. If the Prediction does not exactly match the Ground Truth, compare the functional correctness:\n",
      "   - Run the example test cases provided in the test field mentally or by inspection.\n",
      "   - If all test cases would still pass with the Prediction code, set \"score\" to 1.\n",
      "   - If any test case would fail, set \"score\" to 0.\n",
      "5. If the prediction contains syntax errors or logic unrelated to the task, set \"score\" to 0.\n",
      "6. If the prediction solves the task in a different but correct and efficient way, set \"score\" to 1.\n",
      "\n",
      "### Output a JSON blob with an \"explanation\" field explaining your judgment concisely, and a \"score\" field with value 1 or 0.\n",
      "\n",
      "You should make the judgment based on provided examples.\n",
      "\n",
      "Question: {user_query}\n",
      "Ground Truth:\n",
      "{ground_truth}\n",
      "Prediction:\n",
      "{prediction}\n",
      "Test:\n",
      "{test}\n",
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m processor \u001b[38;5;241m=\u001b[39m CustomEvaluator(evaluator_llm_info \u001b[38;5;241m=\u001b[39m exp_config_data)\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/flotorch-labs/rag-with-bedrockknowledgebases/utils/evaluator.py:195\u001b[0m, in \u001b[0;36mCustomEvaluator.evaluate\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate\u001b[39m(\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    191\u001b[0m         data: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[1;32m    192\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# example to fetch metrics, use like this\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m each \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m--> 195\u001b[0m         prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_system_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43meach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m         payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_payload(prompt)\n\u001b[1;32m    197\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_api_request(payload)\n",
      "File \u001b[0;32m~/flotorch-labs/rag-with-bedrockknowledgebases/utils/evaluator.py:35\u001b[0m, in \u001b[0;36mCustomEvaluator._load_system_prompt\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     33\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m item\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgt_answer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m prediction \u001b[38;5;241m=\u001b[39m item\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m eval_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m eval_prompt\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test'"
     ]
    }
   ],
   "source": [
    "processor = CustomEvaluator(evaluator_llm_info = exp_config_data)\n",
    "results = processor.evaluate(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15768f7b-e1bb-43f4-a4e4-a736f074e372",
   "metadata": {},
   "source": [
    "## Save results to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "feb01294-86c6-4f9d-9744-57c3ab68dd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = './results/evaluation_output.csv'\n",
    "\n",
    "# Check if 'sagemaker_cost' exists in any item\n",
    "include_sagemaker_cost = any('sagemaker_cost' in item for item in results)\n",
    "include_inference_cost = any('inference_cost' in item for item in results)\n",
    "\n",
    "fieldnames=['question', 'answer', 'inputTokens', 'outputTokens', 'totalTokens', 'latencyMs', 'ground answer','message','score']\n",
    "\n",
    "if include_sagemaker_cost:\n",
    "    fieldnames.insert(fieldnames.index('message'), 'sagemaker_cost')  # Insert before 'ground answer'\n",
    "\n",
    "if include_inference_cost:\n",
    "    fieldnames.insert(fieldnames.index('message'), 'bedrock_input_cost')  # Insert before 'ground answer'\n",
    "    fieldnames.insert(fieldnames.index('message'), 'bedrock_output_cost')  # Insert before 'ground answer'\n",
    "    \n",
    "\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for _id, item in enumerate(results):\n",
    "        answer_metadata = item.get('answer_metadata', {})\n",
    "        response = item.get('response', {})\n",
    "\n",
    "        row = {\n",
    "            'question': item.get('question', ''),\n",
    "            'answer': item.get('answer', ''),\n",
    "            'inputTokens': answer_metadata.get('inputTokens', ''),\n",
    "            'outputTokens': answer_metadata.get('outputTokens', ''),\n",
    "            'totalTokens': answer_metadata.get('totalTokens', ''),\n",
    "            'latencyMs': answer_metadata.get('latencyMs', ''),\n",
    "            'ground answer': item.get('gt_answer', ''),\n",
    "            'message': response.get('message', ''),\n",
    "            'score': response.get('score', ''),\n",
    "        }\n",
    "\n",
    "        if include_sagemaker_cost:\n",
    "            sagemaker_cost = item.get('sagemaker_cost', {})\n",
    "            row['sagemaker_cost'] = sagemaker_cost.get('sagemaker_cost', '')\n",
    "        if include_inference_cost:\n",
    "            inference_cost = item.get('inference_cost', {})\n",
    "            row['bedrock_input_cost'] = inference_cost.get('inference_input_cost', '')\n",
    "            row['bedrock_output_cost'] = inference_cost.get('inference_output_cost', '')\n",
    "\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b928b-c5b3-41b5-a1ad-f6ce6f0a2dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ecc0a-02fd-45a5-aed0-da9649267b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f43b757-20be-4efc-b0f5-4de00d97ed67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
