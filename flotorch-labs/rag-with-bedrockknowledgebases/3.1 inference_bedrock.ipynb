{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3afd887e-afe6-4dc8-8f45-328c1c0a86ef",
   "metadata": {},
   "source": [
    "# Retrieval and Generation with Bedrock Foundational Models\n",
    "\n",
    "### Overview  \n",
    "This notebook demonstrates how to perform retrieval-augmented generation (RAG) using Amazon Bedrock's foundational models. It covers retrieving relevant documents from a knowledge base and generating responses based on the retrieved context.\n",
    "\n",
    "### Build your own Retrieval Augmented Generation (RAG) system\n",
    "When constructing your own retrieval augmented generation (RAG) system, you can leverage a retriever system and a generator system. The retriever can be an embedding model that identifies the relevant chunks from the vector database based on similarity scores. The generator can be a Large Language Model (LLM) that utilizes the model's capability to answer questions based on the retrieved results (also known as chunks). In the following sections, we will provide additional tips on how to optimize the prompts for your RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427d8982-69f2-4705-b402-590a0252e5b2",
   "metadata": {},
   "source": [
    "# ğŸ” Retrieval in Flotorch\n",
    "\n",
    "[Flotorch](https://www.flotorch.ai/) is a real-time Retrieval-Augmented Generation (RAG) orchestration engine designed to streamline operational complexity and enhance observability in deploying AI workflows.\n",
    "\n",
    "In Flotorch, **retrieval** refers to the process of fetching relevant information from external knowledge bases to augment the responses generated by language models. This ensures that the AI system provides accurate, timely, and context-aware answers by combining its pre-trained knowledge with up-to-date external data.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ Key Components of Retrieval in Flotorch\n",
    "\n",
    "1. **Retriever**  \n",
    "   Searches external databases or knowledge sources to find relevant information based on the user's query.\n",
    "\n",
    "2. **Augmentation**  \n",
    "   Incorporates the retrieved data into the model's input to enhance the quality and relevance of the generated response.\n",
    "\n",
    "3. **Generator**  \n",
    "   Synthesizes a response by integrating the retrieved information with the model's existing knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Benefits of Retrieval in Flotorch\n",
    "\n",
    "- **Enhanced Accuracy**  \n",
    "  Accesses real-time data to minimize the risk of outdated or incorrect information.\n",
    "\n",
    "- **Contextual Understanding**  \n",
    "  Provides responses that are tailored to the specific query, ensuring relevance and usefulness.\n",
    "\n",
    "- **Scalability**  \n",
    "  Efficiently handles large datasets and complex queries.\n",
    "\n",
    "- **Cost-Effectiveness**  \n",
    "  Reduces the need for frequent retraining by dynamically pulling in fresh data.\n",
    "\n",
    "---\n",
    "\n",
    "This retrieval mechanism is integral to Flotorch's ability to deliver precise and context-aware AI solutions across various industries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c114e690-2fef-4dc8-8a37-f909e512dc56",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 1: load aws variables created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5381fe1f-8477-421a-83ea-a498f1780662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accountNumber': '677276078734',\n",
       " 'regionName': 'us-east-1',\n",
       " 'collectionArn': 'arn:aws:aoss:us-east-1:677276078734:collection/h4x23xd1thd0kpl13b67',\n",
       " 'collectionId': 'h4x23xd1thd0kpl13b67',\n",
       " 'vectorIndexName': 'ws-index-fixed',\n",
       " 'bedrockExecutionRoleArn': 'arn:aws:iam::677276078734:role/advanced-rag-workshop-bedrock_execution_role-us-east-1',\n",
       " 's3Bucket': '677276078734-us-east-1-advanced-rag-workshop',\n",
       " 's3_ground_truth_path': 's3://677276078734-us-east-1-advanced-rag-workshop/ground_truth_data/kbqa_questions_answers.json',\n",
       " 'kbFixedChunk': 'TJSZIWHAIM'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"./results/variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb3777-e1ba-4686-9a76-7faed4fd9f8b",
   "metadata": {},
   "source": [
    "## Load Prompt json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5093f2d-ff0b-44d1-8dee-5415e039a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_file_path = './data/prompt.json'\n",
    "with open(prompt_file_path, 'r') as f:\n",
    "    prompt = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08890a23-c68e-4064-9505-dce141088033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system_prompt': 'You are a world-class Python coding assistant. Your task is to read a function definition that includes its name, type annotations, and docstring with examples, and then generate the full implementation **only inside the function body**, without repeating the function header or docstring.\\n\\nYour response should contain:\\n- Only the indented code inside the function body.\\n- No explanation or extra text.\\n- Pure Python, assuming the imports already exist.\\n- Efficient, clean, and readable logic.',\n",
       " 'examples': [{'question': '\\n\\ndef greatest_common_divisor(a: int, b: int) -> int:\\n    \"\"\" Return a greatest common divisor of two integers a and b\\n    >>> greatest_common_divisor(3, 5)\\n    1\\n    >>> greatest_common_divisor(25, 15)\\n    5\\n    \"\"\"\\n',\n",
       "   'answer': '    while b:\\n        a, b = b, a % b\\n    return a\\n'},\n",
       "  {'question': 'from typing import List\\n\\n\\ndef all_prefixes(string: str) -> List[str]:\\n    \"\"\" Return list of all prefixes from shortest to longest of the input string\\n    >>> all_prefixes(\\'abc\\')\\n    [\\'a\\', \\'ab\\', \\'abc\\']\\n    \"\"\"\\n',\n",
       "   'answer': '    result = []\\n\\n    for i in range(len(string)):\\n        result.append(string[:i+1])\\n    return result\\n'},\n",
       "  {'question': '\\n\\ndef string_sequence(n: int) -> str:\\n    \"\"\" Return a string containing space-delimited numbers starting from 0 upto n inclusive.\\n    >>> string_sequence(0)\\n    \\'0\\'\\n    >>> string_sequence(5)\\n    \\'0 1 2 3 4 5\\'\\n    \"\"\"\\n',\n",
       "   'answer': \"    return ' '.join([str(x) for x in range(n + 1)])\\n\"}],\n",
       " 'user_prompt': 'Now implement the function body based on the definition and docstring provided:'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a199fd02-48ac-42d4-ad88-96682f3e3e01",
   "metadata": {},
   "source": [
    "## Sample experiment JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19762bd0-5991-4b6a-8f95-2017e8b786f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config_data = {\n",
    "            \"temp_retrieval_llm\": \"0.1\",\n",
    "            \"gt_data\": \"s3://flotorch-benchmarking/humaneval/test_questions.json\",\n",
    "            \"rerank_model_id\": \"none\",\n",
    "            \"embedding_model\": \"amazon.titan-embed-text-v2:0\",\n",
    "            \"bedrock_knowledge_base\": False,\n",
    "            \"kb_data\": False,\n",
    "            \"retrieval_service\": \"bedrock\",\n",
    "            \"knn_num\": \"3\",\n",
    "            \"knowledge_base\": False,\n",
    "            \"retrieval_model\": \"us.amazon.nova-pro-v1:0\",\n",
    "            \"index_id\": variables['vectorIndexName'],\n",
    "            \"gateway_api_key\": \"\",\n",
    "            \"vector_dimension\": \"1024\",\n",
    "            \"gateway_enabled\": False,\n",
    "            \"gateway_url\": \"\",\n",
    "            \"chunking_strategy\": \"Fixed\",\n",
    "            \"aws_region\": \"us-east-1\",\n",
    "            \"n_shot_prompt_guide_obj\": prompt,\n",
    "            \"n_shot_prompts\": 1\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f53614-8184-4a99-b5c0-a1dcbce28428",
   "metadata": {},
   "source": [
    "## ğŸ” Load env config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21ed30e6-fe7b-49c4-b46e-97728c584837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.config.env_config_provider import EnvConfigProvider\n",
    "from flotorch_core.config.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60ad8cc3-758f-4487-bffd-7de23fda906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config_provider = EnvConfigProvider()\n",
    "config = Config(env_config_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12371288-c445-42e2-baee-8ac5ee3863aa",
   "metadata": {},
   "source": [
    "### Load Retriver function and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd348a6b-7fd2-4b11-ba90-f2a1a4a73cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in environment variables.\n",
      "/home/chethan/.local/lib/python3.10/site-packages/pydantic/_internal/_fields.py:172: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n",
      "INFO:botocore.credentials:Found credentials in environment variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/chethan/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from flotorch_core.storage.storage_provider_factory import StorageProviderFactory\n",
    "from flotorch_core.reader.json_reader import JSONReader\n",
    "from flotorch_core.storage.db.vector.vector_storage_factory import VectorStorageFactory\n",
    "from flotorch_core.inferencer.inferencer_provider_factory import InferencerProviderFactory\n",
    "from flotorch_core.embedding.embedding_registry import embedding_registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19275dfc",
   "metadata": {},
   "source": [
    "### Initialize storage provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d922ecf0-a40f-4c19-8785-cea9e01670e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_data = exp_config_data['gt_data']\n",
    "storage = StorageProviderFactory.create_storage_provider(gt_data)\n",
    "gt_data_path = storage.get_path(gt_data)\n",
    "json_reader = JSONReader(storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36e755",
   "metadata": {},
   "source": [
    "### Setting embedding to None if bedrock KB is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80bb60cb-63d0-4213-b997-77a1f7441566",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_config_data.get(\"knowledge_base\", False) and not exp_config_data.get(\"bedrock_knowledge_base\", False):\n",
    "    embedding_class = embedding_registry.get_model(exp_config_data.get(\"embedding_model\"))\n",
    "    embedding = embedding_class(\n",
    "        exp_config_data.get(\"embedding_model\"), \n",
    "        exp_config_data.get(\"aws_region\"), \n",
    "        int(exp_config_data.get(\"vector_dimension\")))\n",
    "    is_opensearch_required = True\n",
    "else:\n",
    "    embedding = None\n",
    "    is_opensearch_required = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd97d675-b399-4761-b7a2-b0c197241c39",
   "metadata": {},
   "source": [
    "## ğŸ—ƒï¸ Vector Storage Initialization\n",
    "\n",
    "This section initializes the `VectorStorage` component using a factory method that dynamically selects the appropriate vector storage backend (e.g., OpenSearch, Bedrock Knowledge Base) based on the experimental configuration.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ› ï¸ `VectorStorageFactory.create_vector_storage(...)`\n",
    "\n",
    "Creates an instance of vector storage using configuration flags and credentials.\n",
    "\n",
    "- **Parameters:**\n",
    "  - `knowledge_base`: *(bool)* â€“ Whether a knowledge base is used as a backend.\n",
    "  - `use_bedrock_kb`: *(bool)* â€“ If set, uses AWS Bedrock Knowledge Base.\n",
    "  - `embedding`: *(BaseEmbedding)* â€“ Embedding generator to use for vector creation.\n",
    "  - `opensearch_host`: *(str | None)* â€“ OpenSearch host (set if required).\n",
    "  - `opensearch_port`: *(int | None)* â€“ OpenSearch port (set if required).\n",
    "  - `opensearch_username`: *(str | None)* â€“ OpenSearch authentication username.\n",
    "  - `opensearch_password`: *(str | None)* â€“ OpenSearch authentication password.\n",
    "  - `index_id`: *(str | None)* â€“ Identifier for the index to be used.\n",
    "  - `knowledge_base_id`: *(str | None)* â€“ ID of the Bedrock knowledge base.\n",
    "  - `aws_region`: *(str | None)* â€“ AWS region for Bedrock and related services.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Dynamic Backend Selection\n",
    "\n",
    "The factory method chooses the backend as follows:\n",
    "\n",
    "- If `bedrock_knowledge_base` is enabled â†’ connects to **Bedrock KB**.\n",
    "- Else if `knowledge_base` is enabled â†’ connects to **custom knowledge base**.\n",
    "- Else if `is_opensearch_required` is true â†’ initializes **OpenSearch** with provided credentials.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Result\n",
    "\n",
    "Returns a configured `VectorStorage` instance ready for:\n",
    "- KNN-based vector search\n",
    "- Bedrock KB search\n",
    "- Integration into QA or retrieval pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ad13e",
   "metadata": {},
   "source": [
    "### Initialize vector storage with configuration for embedding and optional OpenSearch/Bedrock KB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79351c77-52b5-4238-8b7c-a33dae9f880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_storage = VectorStorageFactory.create_vector_storage(\n",
    "                knowledge_base=exp_config_data.get(\"knowledge_base\", False),\n",
    "                use_bedrock_kb=exp_config_data.get(\"bedrock_knowledge_base\", False),\n",
    "                embedding=embedding,\n",
    "                opensearch_host=config.get_opensearch_host() if is_opensearch_required else None,\n",
    "                opensearch_port=config.get_opensearch_port() if is_opensearch_required else None,\n",
    "                opensearch_username='admin',\n",
    "                opensearch_password='Flotorch@123',\n",
    "                index_id=exp_config_data.get(\"index_id\"),\n",
    "                knowledge_base_id=exp_config_data.get(\"kb_data\"),\n",
    "                aws_region=exp_config_data.get(\"aws_region\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2d0ee-4675-4afc-8218-1c2790bf480f",
   "metadata": {},
   "source": [
    "## ğŸ¤– Inferencer Initialization\n",
    "\n",
    "This block initializes the **Inferencer** using a factory method that configures the inference engine for text generation or question answering based on the experimental setup.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ—ï¸ `InferencerProviderFactory.create_inferencer_provider(...)`\n",
    "\n",
    "Creates and returns an appropriate `Inferencer` instance depending on configuration such as API gateway usage, model settings, region, and credentials.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ Parameters\n",
    "\n",
    "- `gateway_enabled`: *(bool)* â€“ Enables API gateway-based invocation if set to `True`.\n",
    "- `gateway_url`: *(str)* â€“ URL endpoint for the API Gateway (e.g., `/api/openai/v1`).\n",
    "- `gateway_api_key`: *(str)* â€“ API key for authenticating requests to the gateway.\n",
    "- `retrieval_service`: *(str)* â€“ Name of the retrieval service (e.g., Bedrock, sagemaker).\n",
    "- `retrieval_model`: *(str)* â€“ The model to use for inference (e.g., `anthropic.claude-v2`).\n",
    "- `aws_region`: *(str)* â€“ AWS region for service provisioning (e.g., `us-east-1`).\n",
    "- `iam_role`: *(str)* â€“ IAM role ARN for Bedrock invocation permissions.\n",
    "- `n_shot_prompts`: *(int)* â€“ Number of few-shot examples to include in prompt.\n",
    "- `temp_retrieval_llm`: *(float)* â€“ Temperature setting for the language model.\n",
    "- `n_shot_prompt_guide_obj`: *(Any)* â€“ Few-shot guide object for prompt engineering.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Behavior\n",
    "\n",
    "- If `gateway_enabled` is `True`, connects to the specified API Gateway using credentials.\n",
    "- If disabled, falls back to direct model invocation through supported services like AWS Bedrock.\n",
    "- Supports dynamic few-shot prompting and custom temperature configuration.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Outcome\n",
    "\n",
    "Returns a fully configured `Inferencer` object capable of generating answers or completions for queries using the selected language model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe940888",
   "metadata": {},
   "source": [
    "### Initialize inferencer provider with configuration for gateway, retrieval service, and AWS integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bddfc720-b994-41e2-8491-35881547f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = InferencerProviderFactory.create_inferencer_provider(\n",
    "                exp_config_data.get(\"gateway_enabled\", False),\n",
    "                f'{exp_config_data.get(\"gateway_url\", \"\")}/api/openai/v1',\n",
    "                exp_config_data.get(\"gateway_api_key\", \"\"),\n",
    "                exp_config_data.get(\"retrieval_service\"),\n",
    "                exp_config_data.get(\"retrieval_model\"), \n",
    "                exp_config_data.get(\"aws_region\"), \n",
    "                'arn:aws:iam::677276078734:role/flotorch-bedrock-role-mainqa',\n",
    "                int(exp_config_data.get(\"n_shot_prompts\", 0)), \n",
    "                float(exp_config_data.get(\"temp_retrieval_llm\", 0)), \n",
    "                exp_config_data.get(\"n_shot_prompt_guide_obj\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e74bb4-5639-4216-a1d1-2b2f0d830e3b",
   "metadata": {},
   "source": [
    "## ğŸ” Reranker Initialization\n",
    "\n",
    "This code conditionally initializes the **`BedrockReranker`**, which reorders retrieved documents based on relevance using a reranking model.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ—ï¸ `BedrockReranker(...)` Initialization\n",
    "\n",
    "The reranker is only instantiated if a valid rerank model ID is provided in the experiment configuration.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ Parameters\n",
    "\n",
    "- `aws_region`: *(str)* â€“ AWS region where the Bedrock reranking model is hosted.\n",
    "- `rerank_model_id`: *(str)* â€“ ID of the Bedrock reranking model to be used.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Behavior\n",
    "\n",
    "- If `rerank_model_id` is **not** `\"none\"` (case-insensitive), a `BedrockReranker` is created.\n",
    "- If the value is `\"none\"`, no reranker is used and the value is set to `None`.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Outcome\n",
    "\n",
    "- A `BedrockReranker` object if reranking is enabled.\n",
    "- Otherwise, `reranker = None`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a20966",
   "metadata": {},
   "source": [
    "### Initialize reranker if a valid rerank model ID is provided in the configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38800c24-aa54-4d67-afcb-0fd5c41c4bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = BedrockReranker(exp_config_data.get(\"aws_region\"), exp_config_data.get(\"rerank_model_id\")) \\\n",
    "                if exp_config_data.get(\"rerank_model_id\").lower() != \"none\" \\\n",
    "                else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c335b9f7",
   "metadata": {},
   "source": [
    "### Load ground truth data in JSON reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af7e2cd3-41d7-42a6-a588-5f7d06a5fcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flotorch_core.storage.s3_storage:Reading data from S3 storage\n"
     ]
    }
   ],
   "source": [
    "## Read ground truth json\n",
    "from pydantic import BaseModel\n",
    "from flotorch_core.chunking.chunking import Chunk\n",
    "class Question(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "    test:str\n",
    "\n",
    "    def get_chunk(self) -> Chunk:\n",
    "        return Chunk(data=self.question)\n",
    "\n",
    "questions_list = json_reader.read_as_model(gt_data_path, Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0992f6f5",
   "metadata": {},
   "source": [
    "### ğŸ¤– Perform vector search for each question chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e8c5c53-9308-4878-8cfc-378e718418c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hierarchical = exp_config_data.get(\"chunking_strategy\") == 'hierarchical'\n",
    "\n",
    "responses_list = []\n",
    "for question in questions_list:\n",
    "    question_chunk = question.get_chunk()\n",
    "    vector_response = vector_storage.search(question_chunk, int(exp_config_data.get(\"knn_num\")), hierarchical)\n",
    "    vector_response_result = vector_response.to_json()['result']\n",
    "    responses_list.append({'question':question, 'question_chunk':question_chunk, 'vector_response':vector_response, 'vector_response_result':vector_response_result, 'response_status':vector_response.status})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aba61c",
   "metadata": {},
   "source": [
    "### ğŸ” Rerank vector responses using the reranker if enabled and response is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a955930e-6e00-44e4-919d-6d6bb0953b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for each_response in responses_list:\n",
    "    response_status = each_response['response_status']\n",
    "    vector_response_result = each_response['vector_response_result']\n",
    "    if reranker and response_status:\n",
    "        vector_response = reranker.rerank_documents(each_response['question_chunk'].data, vector_response_result)\n",
    "        each_response['vector_response'] = vector_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38cd325",
   "metadata": {},
   "source": [
    "### ğŸ§  Generate answers and extract metadata for each response, applying guardrail checks if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c739274-0df4-4708-b5e4-400a088cb928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 15:05:00,209 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-22 15:05:01,783 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-22 15:05:02,847 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-22 15:05:03,427 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-22 15:05:04,194 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-22 15:05:05,381 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-22 15:05:06,248 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-22 15:05:08,002 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-22 15:05:08,584 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-22 15:05:09,460 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-22 15:05:10,344 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-22 15:05:11,413 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-22 15:05:12,387 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-22 15:05:13,360 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-22 15:05:14,141 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-22 15:05:14,817 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-22 15:05:15,456 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-22 15:05:15,985 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-22 15:05:16,944 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-22 15:05:17,813 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n"
     ]
    }
   ],
   "source": [
    "for each_response in responses_list:\n",
    "    response_status = each_response['response_status']\n",
    "    if response_status:\n",
    "        question = each_response['question']\n",
    "        vector_response = each_response['vector_response']\n",
    "        vector_response_result = each_response['vector_response_result']\n",
    "        metadata, answer = inferencer.generate_text(question.question, vector_response_result)\n",
    "        guardrail_blocked = metadata['guardrail_blocked'] if 'guardrail_blocked' in metadata else False\n",
    "        if guardrail_blocked:\n",
    "            answer_metadata = {}\n",
    "        else:\n",
    "            answer_metadata = metadata\n",
    "    else:\n",
    "        answer = metadata['guardrail_output']\n",
    "        metadata = {}\n",
    "        answer_metadata = {}\n",
    "        guardrail_blocked = vector_response.metadata['guardrail_blocked'] if 'guardrail_blocked' in vector_response.metadata else False\n",
    "    each_response['metadata'] = metadata\n",
    "    each_response['answer'] = answer\n",
    "    each_response['answer_metadata'] = answer_metadata\n",
    "    each_response['guardrail_blocked'] = guardrail_blocked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932195d0",
   "metadata": {},
   "source": [
    "### ğŸ“¦ Aggregate final results with question, answer, guardrail assessments, and reference context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3eeaedf-be15-4f47-9ac1-af1023ea8d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n',\n",
       "  'answer': 'for i in range(len(numbers)):\\n    for j in range(i + 1, len(numbers)):\\n        if abs(numbers[i] - numbers[j]) < threshold:\\n            return True\\nreturn False',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 335,\n",
       "   'outputTokens': 47,\n",
       "   'totalTokens': 382,\n",
       "   'latencyMs': 638},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n',\n",
       "  'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\\n\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List\\n\\n\\ndef separate_paren_groups(paren_string: str) -> List[str]:\\n    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\\n    separate those group into separate strings and return the list of those.\\n    Separate groups are balanced (each open brace is properly closed) and not nested within each other\\n    Ignore any spaces in the input string.\\n    >>> separate_paren_groups(\\'( ) (( )) (( )( ))\\')\\n    [\\'()\\', \\'(())\\', \\'(()())\\']\\n    \"\"\"\\n',\n",
       "  'answer': 'import re\\ngroups = re.findall(r\\'\\\\([^()]*\\\\)\\', paren_string.replace(\" \", \"\"))\\nreturn groups',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 321,\n",
       "   'outputTokens': 29,\n",
       "   'totalTokens': 350,\n",
       "   'latencyMs': 494},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': \"    result = []\\n    current_string = []\\n    current_depth = 0\\n\\n    for c in paren_string:\\n        if c == '(':\\n            current_depth += 1\\n            current_string.append(c)\\n        elif c == ')':\\n            current_depth -= 1\\n            current_string.append(c)\\n\\n            if current_depth == 0:\\n                result.append(''.join(current_string))\\n                current_string.clear()\\n\\n    return result\\n\",\n",
       "  'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('(()()) ((())) () ((())()())') == [\\n        '(()())', '((()))', '()', '((())()())'\\n    ]\\n    assert candidate('() (()) ((())) (((())))') == [\\n        '()', '(())', '((()))', '(((())))'\\n    ]\\n    assert candidate('(()(())((())))') == [\\n        '(()(())((())))'\\n    ]\\n    assert candidate('( ) (( )) (( )( ))') == ['()', '(())', '(()())']\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': '\\n\\ndef truncate_number(number: float) -> float:\\n    \"\"\" Given a positive floating point number, it can be decomposed into\\n    and integer part (largest integer smaller than given number) and decimals\\n    (leftover part always smaller than 1).\\n\\n    Return the decimal part of the number.\\n    >>> truncate_number(3.5)\\n    0.5\\n    \"\"\"\\n',\n",
       "  'answer': 'return number - int(number)',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 286,\n",
       "   'outputTokens': 7,\n",
       "   'totalTokens': 293,\n",
       "   'latencyMs': 255},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    return number % 1.0\\n',\n",
       "  'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate(3.5) == 0.5\\n    assert abs(candidate(1.33) - 0.33) < 1e-6\\n    assert abs(candidate(123.456) - 0.456) < 1e-6\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List\\n\\n\\ndef below_zero(operations: List[int]) -> bool:\\n    \"\"\" You\\'re given a list of deposit and withdrawal operations on a bank account that starts with\\n    zero balance. Your task is to detect if at any point the balance of account fallls below zero, and\\n    at that point function should return True. Otherwise it should return False.\\n    >>> below_zero([1, 2, 3])\\n    False\\n    >>> below_zero([1, 2, -4, 5])\\n    True\\n    \"\"\"\\n',\n",
       "  'answer': 'balance = 0\\nfor operation in operations:\\n    balance += operation\\n    if balance < 0:\\n        return True\\nreturn False',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 324,\n",
       "   'outputTokens': 28,\n",
       "   'totalTokens': 352,\n",
       "   'latencyMs': 495},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    balance = 0\\n\\n    for op in operations:\\n        balance += op\\n        if balance < 0:\\n            return True\\n\\n    return False\\n',\n",
       "  'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([]) == False\\n    assert candidate([1, 2, -3, 1, 2, -3]) == False\\n    assert candidate([1, 2, -4, 5, 6]) == True\\n    assert candidate([1, -1, 2, -2, 5, -5, 4, -4]) == False\\n    assert candidate([1, -1, 2, -2, 5, -5, 4, -5]) == True\\n    assert candidate([1, -2, 2, -2, 5, -5, 4, -4]) == True\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List\\n\\n\\ndef mean_absolute_deviation(numbers: List[float]) -> float:\\n    \"\"\" For a given list of input numbers, calculate Mean Absolute Deviation\\n    around the mean of this dataset.\\n    Mean Absolute Deviation is the average absolute difference between each\\n    element and a centerpoint (mean in this case):\\n    MAD = average | x - x_mean |\\n    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])\\n    1.0\\n    \"\"\"\\n',\n",
       "  'answer': 'if not numbers:\\n    return 0.0\\n\\nmean_value = sum(numbers) / len(numbers)\\nabsolute_deviations = [abs(number - mean_value) for number in numbers]\\nmad = sum(absolute_deviations) / len(numbers)\\nreturn mad',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 322,\n",
       "   'outputTokens': 62,\n",
       "   'totalTokens': 384,\n",
       "   'latencyMs': 962},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    mean = sum(numbers) / len(numbers)\\n    return sum(abs(x - mean) for x in numbers) / len(numbers)\\n',\n",
       "  'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert abs(candidate([1.0, 2.0, 3.0]) - 2.0/3.0) < 1e-6\\n    assert abs(candidate([1.0, 2.0, 3.0, 4.0]) - 1.0) < 1e-6\\n    assert abs(candidate([1.0, 2.0, 3.0, 4.0, 5.0]) - 6.0/5.0) < 1e-6\\n\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List\\n\\n\\ndef intersperse(numbers: List[int], delimeter: int) -> List[int]:\\n    \"\"\" Insert a number \\'delimeter\\' between every two consecutive elements of input list `numbers\\'\\n    >>> intersperse([], 4)\\n    []\\n    >>> intersperse([1, 2, 3], 4)\\n    [1, 4, 2, 4, 3]\\n    \"\"\"\\n',\n",
       "  'answer': 'if not numbers:\\n    return []\\n\\nresult = [numbers[0]]\\n\\nfor num in numbers[1:]:\\n    result.append(delimeter)\\n    result.append(num)\\n\\nreturn result',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 301,\n",
       "   'outputTokens': 39,\n",
       "   'totalTokens': 340,\n",
       "   'latencyMs': 578},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    if not numbers:\\n        return []\\n\\n    result = []\\n\\n    for n in numbers[:-1]:\\n        result.append(n)\\n        result.append(delimeter)\\n\\n    result.append(numbers[-1])\\n\\n    return result\\n',\n",
       "  'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([], 7) == []\\n    assert candidate([5, 6, 3, 2], 8) == [5, 8, 6, 8, 3, 8, 2]\\n    assert candidate([2, 2, 2], 2) == [2, 2, 2, 2, 2]\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List\\n\\n\\ndef parse_nested_parens(paren_string: str) -> List[int]:\\n    \"\"\" Input to this function is a string represented multiple groups for nested parentheses separated by spaces.\\n    For each of the group, output the deepest level of nesting of parentheses.\\n    E.g. (()()) has maximum two levels of nesting while ((())) has three.\\n\\n    >>> parse_nested_parens(\\'(()()) ((())) () ((())()())\\')\\n    [2, 3, 1, 3]\\n    \"\"\"\\n',\n",
       "  'answer': \"import re\\ndef parse_nested_parens(paren_string: str) -> List[int]:\\n    groups = re.findall(r'\\\\([^()]*\\\\)', paren_string)\\n    depths = []\\n    for group in groups:\\n        depth = 0\\n        current_depth = 0\\n        for char in group:\\n            if char == '(':\\n                current_depth += 1\\n                if current_depth > depth:\\n                    depth = current_depth\\n            elif char == ')':\\n                current_depth -= 1\\n        depths.append(depth)\\n    return depths\",\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 317,\n",
       "   'outputTokens': 126,\n",
       "   'totalTokens': 443,\n",
       "   'latencyMs': 1487},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': \"    def parse_paren_group(s):\\n        depth = 0\\n        max_depth = 0\\n        for c in s:\\n            if c == '(':\\n                depth += 1\\n                max_depth = max(depth, max_depth)\\n            else:\\n                depth -= 1\\n\\n        return max_depth\\n\\n    return [parse_paren_group(x) for x in paren_string.split(' ') if x]\\n\",\n",
       "  'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('(()()) ((())) () ((())()())') == [2, 3, 1, 3]\\n    assert candidate('() (()) ((())) (((())))') == [1, 2, 3, 4]\\n    assert candidate('(()(())((())))') == [4]\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List\\n\\n\\ndef filter_by_substring(strings: List[str], substring: str) -> List[str]:\\n    \"\"\" Filter an input list of strings only for ones that contain given substring\\n    >>> filter_by_substring([], \\'a\\')\\n    []\\n    >>> filter_by_substring([\\'abc\\', \\'bacd\\', \\'cde\\', \\'array\\'], \\'a\\')\\n    [\\'abc\\', \\'bacd\\', \\'array\\']\\n    \"\"\"\\n',\n",
       "  'answer': 'return [s for s in strings if substring in s]',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 305,\n",
       "   'outputTokens': 16,\n",
       "   'totalTokens': 321,\n",
       "   'latencyMs': 320},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    return [x for x in strings if substring in x]\\n',\n",
       "  'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([], 'john') == []\\n    assert candidate(['xxx', 'asd', 'xxy', 'john doe', 'xxxAAA', 'xxx'], 'xxx') == ['xxx', 'xxxAAA', 'xxx']\\n    assert candidate(['xxx', 'asd', 'aaaxxy', 'john doe', 'xxxAAA', 'xxx'], 'xx') == ['xxx', 'aaaxxy', 'xxxAAA', 'xxx']\\n    assert candidate(['grunt', 'trumpet', 'prune', 'gruesome'], 'run') == ['grunt', 'prune']\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List, Tuple\\n\\n\\ndef sum_product(numbers: List[int]) -> Tuple[int, int]:\\n    \"\"\" For a given list of integers, return a tuple consisting of a sum and a product of all the integers in a list.\\n    Empty sum should be equal to 0 and empty product should be equal to 1.\\n    >>> sum_product([])\\n    (0, 1)\\n    >>> sum_product([1, 2, 3, 4])\\n    (10, 24)\\n    \"\"\"\\n',\n",
       "  'answer': 'total_sum = sum(numbers)\\nproduct = 1\\nfor number in numbers:\\n    product *= number\\nreturn (total_sum, product)',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 320,\n",
       "   'outputTokens': 31,\n",
       "   'totalTokens': 351,\n",
       "   'latencyMs': 547},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    sum_value = 0\\n    prod_value = 1\\n\\n    for n in numbers:\\n        sum_value += n\\n        prod_value *= n\\n    return sum_value, prod_value\\n',\n",
       "  'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([]) == (0, 1)\\n    assert candidate([1, 1, 1]) == (3, 1)\\n    assert candidate([100, 0]) == (100, 0)\\n    assert candidate([3, 5, 7]) == (3 + 5 + 7, 3 * 5 * 7)\\n    assert candidate([10]) == (10, 10)\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List, Tuple\\n\\n\\ndef rolling_max(numbers: List[int]) -> List[int]:\\n    \"\"\" From a given list of integers, generate a list of rolling maximum element found until given moment\\n    in the sequence.\\n    >>> rolling_max([1, 2, 3, 2, 3, 4, 2])\\n    [1, 2, 3, 3, 3, 4, 4]\\n    \"\"\"\\n',\n",
       "  'answer': 'result = []\\ncurrent_max = None\\n\\nfor num in numbers:\\n    if current_max is None or num > current_max:\\n        current_max = num\\n    result.append(current_max)\\n\\nreturn result',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 305,\n",
       "   'outputTokens': 46,\n",
       "   'totalTokens': 351,\n",
       "   'latencyMs': 639},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    running_max = None\\n    result = []\\n\\n    for n in numbers:\\n        if running_max is None:\\n            running_max = n\\n        else:\\n            running_max = max(running_max, n)\\n\\n        result.append(running_max)\\n\\n    return result\\n',\n",
       "  'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([]) == []\\n    assert candidate([1, 2, 3, 4]) == [1, 2, 3, 4]\\n    assert candidate([4, 3, 2, 1]) == [4, 4, 4, 4]\\n    assert candidate([3, 2, 3, 100, 3]) == [3, 3, 3, 100, 100]\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': '\\n\\ndef is_palindrome(string: str) -> bool:\\n    \"\"\" Test if given string is a palindrome \"\"\"\\n    return string == string[::-1]\\n\\n\\ndef make_palindrome(string: str) -> str:\\n    \"\"\" Find the shortest palindrome that begins with a supplied string.\\n    Algorithm idea is simple:\\n    - Find the longest postfix of supplied string that is a palindrome.\\n    - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix.\\n    >>> make_palindrome(\\'\\')\\n    \\'\\'\\n    >>> make_palindrome(\\'cat\\')\\n    \\'catac\\'\\n    >>> make_palindrome(\\'cata\\')\\n    \\'catac\\'\\n    \"\"\"\\n',\n",
       "  'answer': 'if not string:\\n    return string\\n\\nfor i in range(len(string), 0, -1):\\n    if is_palindrome(string[i - 1:]):\\n        prefix = string[:i - 1]\\n        return string + prefix[::-1]',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 356,\n",
       "   'outputTokens': 58,\n",
       "   'totalTokens': 414,\n",
       "   'latencyMs': 754},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': \"    if not string:\\n        return ''\\n\\n    beginning_of_suffix = 0\\n\\n    while not is_palindrome(string[beginning_of_suffix:]):\\n        beginning_of_suffix += 1\\n\\n    return string + string[:beginning_of_suffix][::-1]\\n\",\n",
       "  'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('') == ''\\n    assert candidate('x') == 'x'\\n    assert candidate('xyz') == 'xyzyx'\\n    assert candidate('xyx') == 'xyx'\\n    assert candidate('jerry') == 'jerryrrej'\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List\\n\\n\\ndef string_xor(a: str, b: str) -> str:\\n    \"\"\" Input are two strings a and b consisting only of 1s and 0s.\\n    Perform binary XOR on these inputs and return result also as a string.\\n    >>> string_xor(\\'010\\', \\'110\\')\\n    \\'100\\'\\n    \"\"\"\\n',\n",
       "  'answer': 'result = \"\"\\n\\nfor i in range(len(a)):\\n    if a[i] == b[i]:\\n        result += \\'0\\'\\n    else:\\n        result += \\'1\\'\\n\\nreturn result',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 286,\n",
       "   'outputTokens': 40,\n",
       "   'totalTokens': 326,\n",
       "   'latencyMs': 638},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': \"    def xor(i, j):\\n        if i == j:\\n            return '0'\\n        else:\\n            return '1'\\n\\n    return ''.join(xor(x, y) for x, y in zip(a, b))\\n\",\n",
       "  'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('111000', '101010') == '010010'\\n    assert candidate('1', '1') == '0'\\n    assert candidate('0101', '0000') == '0101'\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List, Optional\\n\\n\\ndef longest(strings: List[str]) -> Optional[str]:\\n    \"\"\" Out of list of strings, return the longest one. Return the first one in case of multiple\\n    strings of the same length. Return None in case the input list is empty.\\n    >>> longest([])\\n\\n    >>> longest([\\'a\\', \\'b\\', \\'c\\'])\\n    \\'a\\'\\n    >>> longest([\\'a\\', \\'bb\\', \\'ccc\\'])\\n    \\'ccc\\'\\n    \"\"\"\\n',\n",
       "  'answer': 'if not strings:\\n    return None\\nlongest_str = strings[0]\\nfor s in strings:\\n    if len(s) > len(longest_str):\\n        longest_str = s\\nreturn longest_str',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 308,\n",
       "   'outputTokens': 47,\n",
       "   'totalTokens': 355,\n",
       "   'latencyMs': 664},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    if not strings:\\n        return None\\n\\n    maxlen = max(len(x) for x in strings)\\n    for s in strings:\\n        if len(s) == maxlen:\\n            return s\\n',\n",
       "  'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([]) == None\\n    assert candidate(['x', 'y', 'z']) == 'x'\\n    assert candidate(['x', 'yyy', 'zzzz', 'www', 'kkkk', 'abc']) == 'zzzz'\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': '\\n\\ndef greatest_common_divisor(a: int, b: int) -> int:\\n    \"\"\" Return a greatest common divisor of two integers a and b\\n    >>> greatest_common_divisor(3, 5)\\n    1\\n    >>> greatest_common_divisor(25, 15)\\n    5\\n    \"\"\"\\n',\n",
       "  'answer': '```python\\nwhile b:\\n    a, b = b, a % b\\nreturn a\\n```',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 274,\n",
       "   'outputTokens': 21,\n",
       "   'totalTokens': 295,\n",
       "   'latencyMs': 403},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    while b:\\n        a, b = b, a % b\\n    return a\\n',\n",
       "  'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate(3, 7) == 1\\n    assert candidate(10, 15) == 5\\n    assert candidate(49, 14) == 7\\n    assert candidate(144, 60) == 12\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List\\n\\n\\ndef all_prefixes(string: str) -> List[str]:\\n    \"\"\" Return list of all prefixes from shortest to longest of the input string\\n    >>> all_prefixes(\\'abc\\')\\n    [\\'a\\', \\'ab\\', \\'abc\\']\\n    \"\"\"\\n',\n",
       "  'answer': '```python\\n[string[:i+1] for i in range(len(string))]\\n```',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 263,\n",
       "   'outputTokens': 21,\n",
       "   'totalTokens': 284,\n",
       "   'latencyMs': 416},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    result = []\\n\\n    for i in range(len(string)):\\n        result.append(string[:i+1])\\n    return result\\n',\n",
       "  'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('') == []\\n    assert candidate('asdfgh') == ['a', 'as', 'asd', 'asdf', 'asdfg', 'asdfgh']\\n    assert candidate('WWW') == ['W', 'WW', 'WWW']\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': '\\n\\ndef string_sequence(n: int) -> str:\\n    \"\"\" Return a string containing space-delimited numbers starting from 0 upto n inclusive.\\n    >>> string_sequence(0)\\n    \\'0\\'\\n    >>> string_sequence(5)\\n    \\'0 1 2 3 4 5\\'\\n    \"\"\"\\n',\n",
       "  'answer': \"return ' '.join([str(i) for i in range(n+1)])\",\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 271,\n",
       "   'outputTokens': 20,\n",
       "   'totalTokens': 291,\n",
       "   'latencyMs': 404},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': \"    return ' '.join([str(x) for x in range(n + 1)])\\n\",\n",
       "  'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate(0) == '0'\\n    assert candidate(3) == '0 1 2 3'\\n    assert candidate(10) == '0 1 2 3 4 5 6 7 8 9 10'\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': '\\n\\ndef count_distinct_characters(string: str) -> int:\\n    \"\"\" Given a string, find out how many distinct characters (regardless of case) does it consist of\\n    >>> count_distinct_characters(\\'xyzXYZ\\')\\n    3\\n    >>> count_distinct_characters(\\'Jerry\\')\\n    4\\n    \"\"\"\\n',\n",
       "  'answer': 'return len(set(string.lower()))',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 271,\n",
       "   'outputTokens': 9,\n",
       "   'totalTokens': 280,\n",
       "   'latencyMs': 270},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    return len(set(string.lower()))\\n',\n",
       "  'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('') == 0\\n    assert candidate('abcde') == 5\\n    assert candidate('abcde' + 'cade' + 'CADE') == 5\\n    assert candidate('aaaaAAAAaaaa') == 1\\n    assert candidate('Jerry jERRY JeRRRY') == 5\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List\\n\\n\\ndef parse_music(music_string: str) -> List[int]:\\n    \"\"\" Input to this function is a string representing musical notes in a special ASCII format.\\n    Your task is to parse this string and return list of integers corresponding to how many beats does each\\n    not last.\\n\\n    Here is a legend:\\n    \\'o\\' - whole note, lasts four beats\\n    \\'o|\\' - half note, lasts two beats\\n    \\'.|\\' - quater note, lasts one beat\\n\\n    >>> parse_music(\\'o o| .| o| o| .| .| .| .| o o\\')\\n    [4, 2, 1, 2, 2, 1, 1, 1, 1, 4, 4]\\n    \"\"\"\\n',\n",
       "  'answer': \"beats = []\\nfor note in music_string.split():\\n    if note == 'o':\\n        beats.append(4)\\n    elif note == 'o|':\\n        beats.append(2)\\n    elif note == '.|':\\n        beats.append(1)\\nreturn beats\",\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 376,\n",
       "   'outputTokens': 57,\n",
       "   'totalTokens': 433,\n",
       "   'latencyMs': 740},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': \"    note_map = {'o': 4, 'o|': 2, '.|': 1}\\n    return [note_map[x] for x in music_string.split(' ') if x]\\n\",\n",
       "  'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('') == []\\n    assert candidate('o o o o') == [4, 4, 4, 4]\\n    assert candidate('.| .| .| .|') == [1, 1, 1, 1]\\n    assert candidate('o| o| .| .| o o o o') == [2, 2, 1, 1, 4, 4, 4, 4]\\n    assert candidate('o| .| o| .| o o| o o|') == [2, 1, 2, 1, 4, 2, 4, 2]\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': '\\n\\ndef how_many_times(string: str, substring: str) -> int:\\n    \"\"\" Find how many times a given substring can be found in the original string. Count overlaping cases.\\n    >>> how_many_times(\\'\\', \\'a\\')\\n    0\\n    >>> how_many_times(\\'aaa\\', \\'a\\')\\n    3\\n    >>> how_many_times(\\'aaaa\\', \\'aa\\')\\n    3\\n    \"\"\"\\n',\n",
       "  'answer': 'count = start = 0\\nwhile True:\\n    start = string.find(substring, start) + 1\\n    if start > 0:\\n        count += 1\\n    else:\\n        return count',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 296,\n",
       "   'outputTokens': 44,\n",
       "   'totalTokens': 340,\n",
       "   'latencyMs': 649},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    times = 0\\n\\n    for i in range(len(string) - len(substring) + 1):\\n        if string[i:i+len(substring)] == substring:\\n            times += 1\\n\\n    return times\\n',\n",
       "  'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('', 'x') == 0\\n    assert candidate('xyxyxyx', 'x') == 4\\n    assert candidate('cacacacac', 'cac') == 4\\n    assert candidate('john doe', 'john') == 1\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List\\n\\n\\ndef sort_numbers(numbers: str) -> str:\\n    \"\"\" Input is a space-delimited string of numberals from \\'zero\\' to \\'nine\\'.\\n    Valid choices are \\'zero\\', \\'one\\', \\'two\\', \\'three\\', \\'four\\', \\'five\\', \\'six\\', \\'seven\\', \\'eight\\' and \\'nine\\'.\\n    Return the string with numbers sorted from smallest to largest\\n    >>> sort_numbers(\\'three one five\\')\\n    \\'one three five\\'\\n    \"\"\"\\n',\n",
       "  'answer': \"numerals = {\\n    'zero': 0,\\n    'one': 1,\\n    'two': 2,\\n    'three': 3,\\n    'four': 4,\\n    'five': 5,\\n    'six': 6,\\n    'seven': 7,\\n    'eight': 8,\\n    'nine': 9\\n}\\n\\nwords = numbers.split()\\nsorted_words = sorted(words, key=lambda word: numerals[word])\\nreturn ' '.join(sorted_words)\",\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 314,\n",
       "   'outputTokens': 117,\n",
       "   'totalTokens': 431,\n",
       "   'latencyMs': 1414},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': \"    value_map = {\\n        'zero': 0,\\n        'one': 1,\\n        'two': 2,\\n        'three': 3,\\n        'four': 4,\\n        'five': 5,\\n        'six': 6,\\n        'seven': 7,\\n        'eight': 8,\\n        'nine': 9\\n    }\\n    return ' '.join(sorted([x for x in numbers.split(' ') if x], key=lambda x: value_map[x]))\\n\",\n",
       "  'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('') == ''\\n    assert candidate('three') == 'three'\\n    assert candidate('three five nine') == 'three five nine'\\n    assert candidate('five zero four seven nine eight') == 'zero four five seven eight nine'\\n    assert candidate('six five four three two one zero') == 'zero one two three four five six'\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for each_response in responses_list:\n",
    "    metadata = each_response['metadata']\n",
    "    vector_response = each_response['vector_response']\n",
    "    vector_response_result = each_response['vector_response_result']\n",
    "    result.append(\n",
    "                {'question':each_response['question'].question,\n",
    "                'answer':each_response['answer'],\n",
    "                'guardrails_output_assessment':metadata['guardrail_output_assessment'] if 'guardrail_output_assessment' in metadata else None,\n",
    "                'guardrails_context_assessment':vector_response.metadata['guardrail_context_assessment'] if 'guardrail_context_assessment' in vector_response.metadata else None,\n",
    "                'guardrails_input_assessment':vector_response.metadata['guardrail_input_assessment'] if 'guardrail_input_assessment' in vector_response.metadata else None,\n",
    "                'guardrails_blocked':each_response['guardrail_blocked'],\n",
    "                'guardrails_block_level':vector_response.metadata['block_level'] if 'block_level' in vector_response.metadata else \"\",\n",
    "                'answer_metadata':each_response['answer_metadata'],\n",
    "                'reference_contexts':[res['text'] for res in vector_response_result] if vector_response_result else [],\n",
    "                'gt_answer':each_response['question'].answer,\n",
    "                'test': each_response['question'].test,\n",
    "                'query_metadata':vector_response.metadata['embedding_metadata'].to_json() if 'embedding_metadata' in vector_response.metadata else None\n",
    "                })\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba6ebdd-2388-4e1e-8681-41de9fedeab4",
   "metadata": {},
   "source": [
    "### ğŸ“¦ Calculate Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9857bc86-5e08-4d26-90a6-cc92bf07f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.cost_calculation import calculate_total_cost\n",
    "total_cost, results = calculate_total_cost(exp_config_data, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84674286",
   "metadata": {},
   "source": [
    "### ğŸ’¾ Save the aggregated results to a JSON file for inference metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dc91fa0-5952-4d83-8372-ed46d47dd91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./results/{exp_config_data['retrieval_service']}_inference_metrics.json\", \"w\") as json_file:\n",
    "    json.dump(results, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f72648-1282-4fe0-b78b-6b9a1bd7dcad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01458e5e-7103-4c68-957a-c67e2213e20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a903173-61d3-4bca-b423-cf92d3c0e5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2fa652-dd59-4f5e-9b73-2af0b79020d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
