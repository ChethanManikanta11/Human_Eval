{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a6fdaec-a86e-47f7-9ed9-3d0242e5a9b3",
   "metadata": {},
   "source": [
    "# ðŸ“Š Evaluation in Flotorch\n",
    "\n",
    "[Flotorch](https://www.flotorch.ai/) provides a comprehensive evaluation framework for Retrieval-Augmented Generation (RAG) systems. It helps assess and compare Large Language Models (LLMs) based on relevance, quality, cost, and performance to support enterprise-grade deployments.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Key Evaluation Features\n",
    "\n",
    "- **Automated LLM Evaluation**  \n",
    "  Flotorch automates evaluation across:\n",
    "  - Relevance\n",
    "  - Fluency\n",
    "  - Robustness\n",
    "  - Cost\n",
    "  - Execution Speed\n",
    "\n",
    "- **Performance Metrics**  \n",
    "  It generates quantitative scores for evaluating how well a model performs across different criteria.\n",
    "\n",
    "- **Cost and Time Insights**  \n",
    "  Offers pricing and latency breakdowns for different LLM setups, enabling cost-effective choices.\n",
    "\n",
    "- **Data-Driven Decision-Making**  \n",
    "  Helps teams align LLM usage with specific application goals, budget, and performance needs.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Evaluation Workflow\n",
    "\n",
    "1. **Experiment Configuration**  \n",
    "   Define models, parameters, and goals for evaluation.\n",
    "\n",
    "2. **Automated Execution**  \n",
    "   Run evaluation pipelines to generate performance data.\n",
    "\n",
    "3. **Results Analysis**  \n",
    "   View dashboards or reports that summarize evaluation results.\n",
    "\n",
    "4. **Expert Evaluation (Optional)**  \n",
    "   Combine automatic evaluation with human review for more nuanced feedback.\n",
    "\n",
    "---\n",
    "\n",
    "This evaluation framework enables continuous monitoring, benchmarking, and optimization of RAG systems using LLMs, helping organizations deploy more reliable and efficient AI solutions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e067c92-f0f4-4135-a351-bf1f7c10d56d",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Invalid control character at: line 2 column 93 (char 94)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_prompt.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(prompt_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 4\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Invalid control character at: line 2 column 93 (char 94)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "prompt_file_path = 'eval_prompt.json'\n",
    "with open(prompt_file_path, 'r') as f:\n",
    "    prompt = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce74afb-8660-4576-b446-2b73f4b38191",
   "metadata": {},
   "source": [
    "## Load experiment config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5ad790b-2ea5-4549-8637-d08e0ff618fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/chethan/flotorch-labs/rag-with-bedrockknowledgebases/data'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7bb7517-ea81-426b-8d2f-2e246970b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config_data = {\n",
    "            \"temp_retrieval_llm\": \"0.1\",\n",
    "            \"retrival_service\": \"bedrock\",\n",
    "            \"eval_retrieval_model\": \"bedrock/cohere.command-r-v1:0\",\n",
    "            \"eval_prompt\": prompt\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54042c51-bdab-4524-922c-d65651d09004",
   "metadata": {},
   "source": [
    "## Load inference metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "982fb77b-f992-4cf9-ba5f-6e8e63941a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f\"./results/{exp_config_data['retrival_service']}_inference_metrics.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b3f7e0c2-87b7-43ed-8c1c-5803fb6abd64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_prompt': 'Assume you are a human expert in grading predictions given by a model. You are given a question and a model prediction. Judge if the prediction matches the ground truth answer by following these steps:\\n1: Take it as granted that the Ground Truth is always correct.\\n2: If the Prediction indicates it is not sure about the answer or it shows that it has insufficient answer, score should be \"0\"; otherwise, go to the next step.\\n3: If the Prediction exactly matches the Ground Truth, score is 1.\\n4: If the Prediction does not exactly match the Ground Truth, go through the following steps and likely give a score as 0.\\n5: If the Ground Truth is a number, score is 1 if and only if the Prediction gives a number that almost exactly matches the ground truth.\\n6: If the Prediction is self-contradictory, score must be 0.\\n7: If the prediction is not answering the question, score must be 0.\\n8: If the prediction is a concise and correct summary of the ground truth, score is 1.\\n10: Otherwise, score is 0.\\n\\n### Output a JSON blob with an \"explanation\" field explaining your answer as short as possible and a score field with value 1 or 0.\\n\\nYou should make the judgment based on provided examples.\\nRun the examples provided in the docstring mentally or via inspection. Run the example tests which are there in test field.\\nExamples:\\nQuestion: \"What are the three main sub-tasks in Knowledge Base Question Answering (KBQA) as identified in the paper?\"\\nGround Truth: \"The three main sub-tasks in KBQA are topic entity detection, entity linking, and relation detection.\"\\nPrediction: \"Sorry, I don\\'t have sufficient information to provide an answer.\"\\nOutput: {{\"score\": 0, \"explanation\": \"The prediction is not sure about the answer.\"}}\\n\\nQuestion: \"What is the role of BERT in the proposed KBQA framework?\"\\nGround Truth: \"BERT is used to improve accuracy across all three sub-tasks by serving as a shared encoder in the multi-task learning framework.\"\\nPrediction: \"BERT is used only for tokenization and does not contribute to the accuracy improvements in any of the sub-tasks within the multi-task learning framework.\"\\nOutput: {{\"score\": 0, \"explanation\": \"The prediction doesn\\'t match with ground truth\"}}\\n\\nQuestion: \"What is the significance of the PRACTIQ dataset in the context of real-world applications?\"\\nGround Truth: \"The PRACTIQ dataset is significant for real-world applications as it introduces practical challenges faced in conversational text-to-SQL systems, such as handling ambiguous and unanswerable queries, thereby aiding in the development of more robust and user-friendly database query interfaces.\"\\nPrediction: \"The PRACTIQ dataset is significant for real-world applications as it introduces practical challenges faced in conversational text-to-SQL systems, such as handling ambiguous and unanswerable queries, thereby aiding in the development of more robust and user-friendly database query interfaces.\"\\nOutput: {\"score\": 1, \"explanation\": \"The prediction matches the ground truth.\"}\\n\\nQuestion: \"How does Amazon evaluate whether a model meets or exceeds a Critical Capability Threshold?\"\\nGround Truth: \"Amazon conducts Critical Capability Evaluations using a combination of automated and human-in-the-loop strategies to assess if models demonstrate capabilities that meet or exceed the defined thresholds.\"\\nPrediction: \"Amazon evaluates whether a model meets or exceeds a Critical Capability Threshold through a combination of automated and human-in-the-loop strategies. These evaluations include:\\n\\n1. **Automated Benchmarks**: Using state-of-the-art public benchmarks and internal proprietary test sets to compare candidate models.\\n2. **Expert Red Teaming**: Engaging specialized firms and academics to test models for safety and security, particularly for risks requiring domain-specific expertise.\\n3. **Uplift Studies**: Conducting studies to assess whether access to a model enhances human capability to perform tasks compared to other existing resources.\"\\nOutput: {\"score\": 1, \"explanation\": \"The prediction matches the ground truth.\"}\\n\\nQuestion: {user_query}\\\\n Ground truth: {ground_truth}\\\\n Prediction: {prediction}\\\\n'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59153913-5f0a-4a5f-8fd6-a7af3f5ae19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a0c847c-dec5-4bb0-a29f-1f74f2ee5d91",
   "metadata": {},
   "source": [
    "## Load Evaluator Class\n",
    "\n",
    "### ðŸ§  Evaluation with `CustomEvaluator`\n",
    "\n",
    "```python\n",
    "processor = CustomEvaluator(evaluator_llm=exp_config_data['eval_retrieval_model'])\n",
    "results = processor.evaluate(data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Step-by-Step Breakdown\n",
    "\n",
    "| Line | Description |\n",
    "|------|-------------|\n",
    "| `processor = CustomEvaluator(...)` | Instantiates a `CustomEvaluator` using a language model specified in the config (`exp_config_data['eval_retrieval_model']`). |\n",
    "| `results = processor.evaluate(data)` | Runs the evaluation on the `data` using the evaluator, returning performance metrics or scoring output. |\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ§© Key Components\n",
    "\n",
    "- **`CustomEvaluator`**: A custom class designed to handle evaluation logic, potentially wrapping RAGAS or similar frameworks.\n",
    "- **`evaluator_llm`**: The evaluation language model (e.g. GPT, Claude, etc.) used for scoring responses.\n",
    "- **`data`**: A list of evaluation items (e.g. questions, answers, reference contexts).\n",
    "- **`results`**: The output from the evaluation â€” typically a dictionary or structured result with metric scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "913229fb-86b2-4da2-9321-a461618a5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluator import CustomEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2d81ee62-cff0-4581-9274-ba047fe199b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'\"score\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m processor \u001b[38;5;241m=\u001b[39m CustomEvaluator(evaluator_llm_info \u001b[38;5;241m=\u001b[39m exp_config_data)\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/flotorch-labs/rag-with-bedrockknowledgebases/utils/evaluator.py:192\u001b[0m, in \u001b[0;36mCustomEvaluator.evaluate\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate\u001b[39m(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    188\u001b[0m         data: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[1;32m    189\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m# example to fetch metrics, use like this\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m each \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m--> 192\u001b[0m         prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_system_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43meach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m         payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_payload(prompt)\n\u001b[1;32m    194\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_api_request(payload)\n",
      "File \u001b[0;32m~/flotorch-labs/rag-with-bedrockknowledgebases/utils/evaluator.py:32\u001b[0m, in \u001b[0;36mCustomEvaluator._load_system_prompt\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     30\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m item\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgt_answer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m prediction \u001b[38;5;241m=\u001b[39m item\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m eval_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m eval_prompt\n",
      "\u001b[0;31mKeyError\u001b[0m: '\"score\"'"
     ]
    }
   ],
   "source": [
    "processor = CustomEvaluator(evaluator_llm_info = exp_config_data)\n",
    "results = processor.evaluate(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15768f7b-e1bb-43f4-a4e4-a736f074e372",
   "metadata": {},
   "source": [
    "## Save results to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb01294-86c6-4f9d-9744-57c3ab68dd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = './results/evaluation_output.csv'\n",
    "\n",
    "# Check if 'sagemaker_cost' exists in any item\n",
    "include_sagemaker_cost = any('sagemaker_cost' in item for item in results)\n",
    "include_inference_cost = any('inference_cost' in item for item in results)\n",
    "\n",
    "fieldnames=['question', 'answer', 'inputTokens', 'outputTokens', 'totalTokens', 'latencyMs', 'ground answer','message','score']\n",
    "\n",
    "if include_sagemaker_cost:\n",
    "    fieldnames.insert(fieldnames.index('message'), 'sagemaker_cost')  # Insert before 'ground answer'\n",
    "\n",
    "if include_inference_cost:\n",
    "    fieldnames.insert(fieldnames.index('message'), 'bedrock_input_cost')  # Insert before 'ground answer'\n",
    "    fieldnames.insert(fieldnames.index('message'), 'bedrock_output_cost')  # Insert before 'ground answer'\n",
    "    \n",
    "\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for _id, item in enumerate(results):\n",
    "        answer_metadata = item.get('answer_metadata', {})\n",
    "        response = item.get('response', {})\n",
    "\n",
    "        row = {\n",
    "            'question': item.get('question', ''),\n",
    "            'answer': item.get('answer', ''),\n",
    "            'inputTokens': answer_metadata.get('inputTokens', ''),\n",
    "            'outputTokens': answer_metadata.get('outputTokens', ''),\n",
    "            'totalTokens': answer_metadata.get('totalTokens', ''),\n",
    "            'latencyMs': answer_metadata.get('latencyMs', ''),\n",
    "            'ground answer': item.get('gt_answer', ''),\n",
    "            'message': response.get('message', ''),\n",
    "            'score': response.get('score', ''),\n",
    "        }\n",
    "\n",
    "        if include_sagemaker_cost:\n",
    "            sagemaker_cost = item.get('sagemaker_cost', {})\n",
    "            row['sagemaker_cost'] = sagemaker_cost.get('sagemaker_cost', '')\n",
    "        if include_inference_cost:\n",
    "            inference_cost = item.get('inference_cost', {})\n",
    "            row['bedrock_input_cost'] = inference_cost.get('inference_input_cost', '')\n",
    "            row['bedrock_output_cost'] = inference_cost.get('inference_output_cost', '')\n",
    "\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b928b-c5b3-41b5-a1ad-f6ce6f0a2dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
