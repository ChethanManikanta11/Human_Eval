eval_prompt = """Assume you are a human expert in grading Python function implementations. You are given a function definition, a ground truth implementation, a model prediction, and example test cases provided in the `test` field. Judge if the model's implementation matches the ground truth by following these steps:

1. Assume the Ground Truth is always correct.
2. If the Prediction is incomplete or shows it does not attempt a real solution, set "score" to 0.
3. If the Prediction exactly matches the Ground Truth implementation, set "score" to 1.
4. If the Prediction does not exactly match the Ground Truth, compare the functional correctness:
   - Run the example test cases provided in the `test` field mentally or by inspection.
   - If all test cases would still pass with the Prediction code, set "score" to 1.
   - If any test case would fail, set "score" to 0.
5. If the prediction contains syntax errors or logic unrelated to the task, set "score" to 0.
6. If the prediction solves the task in a different but correct and efficient way, set "score" to 1.

### Output a JSON blob with an "explanation" field explaining your judgment concisely, and a "score" field with value 1 or 0.

You should make the judgment based on provided examples.

Examples:

Question: "def string_sequence(n: int) -> str:\\n    \\\"\\\"\\\" Return a string containing space-delimited numbers starting from 0 upto n inclusive.\\n    \\\"\\\"\\\""
Ground Truth:
    return ' '.join([str(x) for x in range(n + 1)])
Prediction:
    return ' '.join(str(i) for i in range(n + 1))
Test: ['string_sequence(0) == "0"', 'string_sequence(5) == "0 1 2 3 4 5"']
Output: {{ "score": 1, "explanation": "Prediction is functionally equivalent and passes all test cases." }}

Question: "def all_prefixes(string: str) -> List[str]:\\n    \\\"\\\"\\\" Return list of all prefixes from shortest to longest of the input string\\n    \\\"\\\"\\\""
Ground Truth:
    result = []
    for i in range(len(string)):
        result.append(string[:i+1])
    return result
Prediction:
    return [string[i:] for i in range(len(string))]
Test: ["all_prefixes('abc') == ['a', 'ab', 'abc']"]
Output: {{ "score": 0, "explanation": "Prediction generates suffixes instead of prefixes, so test fails." }}

Question: {question}\\nGround Truth: {ground_truth}\\nPrediction: {prediction}\\nTest: {test}\\n"""
