{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3afd887e-afe6-4dc8-8f45-328c1c0a86ef",
   "metadata": {},
   "source": [
    "# Retrieval and Generation with Bedrock Foundational Models\n",
    "\n",
    "### Overview  \n",
    "This notebook demonstrates how to perform retrieval-augmented generation (RAG) using Amazon Bedrock's foundational models. It covers retrieving relevant documents from a knowledge base and generating responses based on the retrieved context.\n",
    "\n",
    "### Build your own Retrieval Augmented Generation (RAG) system\n",
    "When constructing your own retrieval augmented generation (RAG) system, you can leverage a retriever system and a generator system. The retriever can be an embedding model that identifies the relevant chunks from the vector database based on similarity scores. The generator can be a Large Language Model (LLM) that utilizes the model's capability to answer questions based on the retrieved results (also known as chunks). In the following sections, we will provide additional tips on how to optimize the prompts for your RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427d8982-69f2-4705-b402-590a0252e5b2",
   "metadata": {},
   "source": [
    "# ğŸ” Retrieval in Flotorch\n",
    "\n",
    "[Flotorch](https://www.flotorch.ai/) is a real-time Retrieval-Augmented Generation (RAG) orchestration engine designed to streamline operational complexity and enhance observability in deploying AI workflows.\n",
    "\n",
    "In Flotorch, **retrieval** refers to the process of fetching relevant information from external knowledge bases to augment the responses generated by language models. This ensures that the AI system provides accurate, timely, and context-aware answers by combining its pre-trained knowledge with up-to-date external data.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ Key Components of Retrieval in Flotorch\n",
    "\n",
    "1. **Retriever**  \n",
    "   Searches external databases or knowledge sources to find relevant information based on the user's query.\n",
    "\n",
    "2. **Augmentation**  \n",
    "   Incorporates the retrieved data into the model's input to enhance the quality and relevance of the generated response.\n",
    "\n",
    "3. **Generator**  \n",
    "   Synthesizes a response by integrating the retrieved information with the model's existing knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Benefits of Retrieval in Flotorch\n",
    "\n",
    "- **Enhanced Accuracy**  \n",
    "  Accesses real-time data to minimize the risk of outdated or incorrect information.\n",
    "\n",
    "- **Contextual Understanding**  \n",
    "  Provides responses that are tailored to the specific query, ensuring relevance and usefulness.\n",
    "\n",
    "- **Scalability**  \n",
    "  Efficiently handles large datasets and complex queries.\n",
    "\n",
    "- **Cost-Effectiveness**  \n",
    "  Reduces the need for frequent retraining by dynamically pulling in fresh data.\n",
    "\n",
    "---\n",
    "\n",
    "This retrieval mechanism is integral to Flotorch's ability to deliver precise and context-aware AI solutions across various industries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c114e690-2fef-4dc8-8a37-f909e512dc56",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 1: load aws variables created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5381fe1f-8477-421a-83ea-a498f1780662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accountNumber': '677276078734',\n",
       " 'regionName': 'us-east-1',\n",
       " 'collectionArn': 'arn:aws:aoss:us-east-1:677276078734:collection/h4x23xd1thd0kpl13b67',\n",
       " 'collectionId': 'h4x23xd1thd0kpl13b67',\n",
       " 'vectorIndexName': 'ws-index-fixed',\n",
       " 'bedrockExecutionRoleArn': 'arn:aws:iam::677276078734:role/advanced-rag-workshop-bedrock_execution_role-us-east-1',\n",
       " 's3Bucket': '677276078734-us-east-1-advanced-rag-workshop',\n",
       " 's3_ground_truth_path': 's3://677276078734-us-east-1-advanced-rag-workshop/ground_truth_data/kbqa_questions_answers.json',\n",
       " 'kbFixedChunk': 'TJSZIWHAIM'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"./results/variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb3777-e1ba-4686-9a76-7faed4fd9f8b",
   "metadata": {},
   "source": [
    "## Load Prompt json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5093f2d-ff0b-44d1-8dee-5415e039a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_file_path = './data/prompt.json'\n",
    "with open(prompt_file_path, 'r') as f:\n",
    "    prompt = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a199fd02-48ac-42d4-ad88-96682f3e3e01",
   "metadata": {},
   "source": [
    "## Sample experiment JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19762bd0-5991-4b6a-8f95-2017e8b786f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config_data = {\n",
    "            \"temp_retrieval_llm\": \"0.1\",\n",
    "            \"gt_data\": \"s3://flotorch-benchmarking/humaneval/test_questions.json\",\n",
    "            \"rerank_model_id\": \"none\",\n",
    "            \"embedding_model\": \"amazon.titan-embed-text-v2:0\",\n",
    "            \"bedrock_knowledge_base\": False,\n",
    "            \"kb_data\": False,\n",
    "            \"retrieval_service\": \"bedrock\",\n",
    "            \"knn_num\": \"3\",\n",
    "            \"knowledge_base\": False,\n",
    "            \"retrieval_model\": \"us.amazon.nova-pro-v1:0\",\n",
    "            \"index_id\": variables['vectorIndexName'],\n",
    "            \"gateway_api_key\": \"\",\n",
    "            \"vector_dimension\": \"1024\",\n",
    "            \"gateway_enabled\": False,\n",
    "            \"gateway_url\": \"\",\n",
    "            \"chunking_strategy\": \"Fixed\",\n",
    "            \"aws_region\": \"us-east-1\",\n",
    "            \"n_shot_prompt_guide_obj\": prompt,\n",
    "            \"n_shot_prompts\": 1\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f53614-8184-4a99-b5c0-a1dcbce28428",
   "metadata": {},
   "source": [
    "## ğŸ” Load env config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ed30e6-fe7b-49c4-b46e-97728c584837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.config.env_config_provider import EnvConfigProvider\n",
    "from flotorch_core.config.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ad8cc3-758f-4487-bffd-7de23fda906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config_provider = EnvConfigProvider()\n",
    "config = Config(env_config_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12371288-c445-42e2-baee-8ac5ee3863aa",
   "metadata": {},
   "source": [
    "### Load Retriver function and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd348a6b-7fd2-4b11-ba90-f2a1a4a73cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in environment variables.\n",
      "/home/chethan/.local/lib/python3.10/site-packages/pydantic/_internal/_fields.py:172: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n",
      "INFO:botocore.credentials:Found credentials in environment variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/chethan/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from flotorch_core.storage.storage_provider_factory import StorageProviderFactory\n",
    "from flotorch_core.reader.json_reader import JSONReader\n",
    "from flotorch_core.storage.db.vector.vector_storage_factory import VectorStorageFactory\n",
    "from flotorch_core.inferencer.inferencer_provider_factory import InferencerProviderFactory\n",
    "from flotorch_core.embedding.embedding_registry import embedding_registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19275dfc",
   "metadata": {},
   "source": [
    "### Initialize storage provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d922ecf0-a40f-4c19-8785-cea9e01670e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_data = exp_config_data['gt_data']\n",
    "storage = StorageProviderFactory.create_storage_provider(gt_data)\n",
    "gt_data_path = storage.get_path(gt_data)\n",
    "json_reader = JSONReader(storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36e755",
   "metadata": {},
   "source": [
    "### Setting embedding to None if bedrock KB is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80bb60cb-63d0-4213-b997-77a1f7441566",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_config_data.get(\"knowledge_base\", False) and not exp_config_data.get(\"bedrock_knowledge_base\", False):\n",
    "    embedding_class = embedding_registry.get_model(exp_config_data.get(\"embedding_model\"))\n",
    "    embedding = embedding_class(\n",
    "        exp_config_data.get(\"embedding_model\"), \n",
    "        exp_config_data.get(\"aws_region\"), \n",
    "        int(exp_config_data.get(\"vector_dimension\")))\n",
    "    is_opensearch_required = True\n",
    "else:\n",
    "    embedding = None\n",
    "    is_opensearch_required = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd97d675-b399-4761-b7a2-b0c197241c39",
   "metadata": {},
   "source": [
    "## ğŸ—ƒï¸ Vector Storage Initialization\n",
    "\n",
    "This section initializes the `VectorStorage` component using a factory method that dynamically selects the appropriate vector storage backend (e.g., OpenSearch, Bedrock Knowledge Base) based on the experimental configuration.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ› ï¸ `VectorStorageFactory.create_vector_storage(...)`\n",
    "\n",
    "Creates an instance of vector storage using configuration flags and credentials.\n",
    "\n",
    "- **Parameters:**\n",
    "  - `knowledge_base`: *(bool)* â€“ Whether a knowledge base is used as a backend.\n",
    "  - `use_bedrock_kb`: *(bool)* â€“ If set, uses AWS Bedrock Knowledge Base.\n",
    "  - `embedding`: *(BaseEmbedding)* â€“ Embedding generator to use for vector creation.\n",
    "  - `opensearch_host`: *(str | None)* â€“ OpenSearch host (set if required).\n",
    "  - `opensearch_port`: *(int | None)* â€“ OpenSearch port (set if required).\n",
    "  - `opensearch_username`: *(str | None)* â€“ OpenSearch authentication username.\n",
    "  - `opensearch_password`: *(str | None)* â€“ OpenSearch authentication password.\n",
    "  - `index_id`: *(str | None)* â€“ Identifier for the index to be used.\n",
    "  - `knowledge_base_id`: *(str | None)* â€“ ID of the Bedrock knowledge base.\n",
    "  - `aws_region`: *(str | None)* â€“ AWS region for Bedrock and related services.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Dynamic Backend Selection\n",
    "\n",
    "The factory method chooses the backend as follows:\n",
    "\n",
    "- If `bedrock_knowledge_base` is enabled â†’ connects to **Bedrock KB**.\n",
    "- Else if `knowledge_base` is enabled â†’ connects to **custom knowledge base**.\n",
    "- Else if `is_opensearch_required` is true â†’ initializes **OpenSearch** with provided credentials.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Result\n",
    "\n",
    "Returns a configured `VectorStorage` instance ready for:\n",
    "- KNN-based vector search\n",
    "- Bedrock KB search\n",
    "- Integration into QA or retrieval pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ad13e",
   "metadata": {},
   "source": [
    "### Initialize vector storage with configuration for embedding and optional OpenSearch/Bedrock KB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79351c77-52b5-4238-8b7c-a33dae9f880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_storage = VectorStorageFactory.create_vector_storage(\n",
    "                knowledge_base=exp_config_data.get(\"knowledge_base\", False),\n",
    "                use_bedrock_kb=exp_config_data.get(\"bedrock_knowledge_base\", False),\n",
    "                embedding=embedding,\n",
    "                opensearch_host=config.get_opensearch_host() if is_opensearch_required else None,\n",
    "                opensearch_port=config.get_opensearch_port() if is_opensearch_required else None,\n",
    "                opensearch_username='admin',\n",
    "                opensearch_password='Flotorch@123',\n",
    "                index_id=exp_config_data.get(\"index_id\"),\n",
    "                knowledge_base_id=exp_config_data.get(\"kb_data\"),\n",
    "                aws_region=exp_config_data.get(\"aws_region\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2d0ee-4675-4afc-8218-1c2790bf480f",
   "metadata": {},
   "source": [
    "## ğŸ¤– Inferencer Initialization\n",
    "\n",
    "This block initializes the **Inferencer** using a factory method that configures the inference engine for text generation or question answering based on the experimental setup.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ—ï¸ `InferencerProviderFactory.create_inferencer_provider(...)`\n",
    "\n",
    "Creates and returns an appropriate `Inferencer` instance depending on configuration such as API gateway usage, model settings, region, and credentials.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ Parameters\n",
    "\n",
    "- `gateway_enabled`: *(bool)* â€“ Enables API gateway-based invocation if set to `True`.\n",
    "- `gateway_url`: *(str)* â€“ URL endpoint for the API Gateway (e.g., `/api/openai/v1`).\n",
    "- `gateway_api_key`: *(str)* â€“ API key for authenticating requests to the gateway.\n",
    "- `retrieval_service`: *(str)* â€“ Name of the retrieval service (e.g., Bedrock, sagemaker).\n",
    "- `retrieval_model`: *(str)* â€“ The model to use for inference (e.g., `anthropic.claude-v2`).\n",
    "- `aws_region`: *(str)* â€“ AWS region for service provisioning (e.g., `us-east-1`).\n",
    "- `iam_role`: *(str)* â€“ IAM role ARN for Bedrock invocation permissions.\n",
    "- `n_shot_prompts`: *(int)* â€“ Number of few-shot examples to include in prompt.\n",
    "- `temp_retrieval_llm`: *(float)* â€“ Temperature setting for the language model.\n",
    "- `n_shot_prompt_guide_obj`: *(Any)* â€“ Few-shot guide object for prompt engineering.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Behavior\n",
    "\n",
    "- If `gateway_enabled` is `True`, connects to the specified API Gateway using credentials.\n",
    "- If disabled, falls back to direct model invocation through supported services like AWS Bedrock.\n",
    "- Supports dynamic few-shot prompting and custom temperature configuration.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Outcome\n",
    "\n",
    "Returns a fully configured `Inferencer` object capable of generating answers or completions for queries using the selected language model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe940888",
   "metadata": {},
   "source": [
    "### Initialize inferencer provider with configuration for gateway, retrieval service, and AWS integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bddfc720-b994-41e2-8491-35881547f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = InferencerProviderFactory.create_inferencer_provider(\n",
    "                exp_config_data.get(\"gateway_enabled\", False),\n",
    "                f'{exp_config_data.get(\"gateway_url\", \"\")}/api/openai/v1',\n",
    "                exp_config_data.get(\"gateway_api_key\", \"\"),\n",
    "                exp_config_data.get(\"retrieval_service\"),\n",
    "                exp_config_data.get(\"retrieval_model\"), \n",
    "                exp_config_data.get(\"aws_region\"), \n",
    "                'arn:aws:iam::677276078734:role/flotorch-bedrock-role-mainqa',\n",
    "                int(exp_config_data.get(\"n_shot_prompts\", 0)), \n",
    "                float(exp_config_data.get(\"temp_retrieval_llm\", 0)), \n",
    "                exp_config_data.get(\"n_shot_prompt_guide_obj\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e74bb4-5639-4216-a1d1-2b2f0d830e3b",
   "metadata": {},
   "source": [
    "## ğŸ” Reranker Initialization\n",
    "\n",
    "This code conditionally initializes the **`BedrockReranker`**, which reorders retrieved documents based on relevance using a reranking model.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ—ï¸ `BedrockReranker(...)` Initialization\n",
    "\n",
    "The reranker is only instantiated if a valid rerank model ID is provided in the experiment configuration.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ Parameters\n",
    "\n",
    "- `aws_region`: *(str)* â€“ AWS region where the Bedrock reranking model is hosted.\n",
    "- `rerank_model_id`: *(str)* â€“ ID of the Bedrock reranking model to be used.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Behavior\n",
    "\n",
    "- If `rerank_model_id` is **not** `\"none\"` (case-insensitive), a `BedrockReranker` is created.\n",
    "- If the value is `\"none\"`, no reranker is used and the value is set to `None`.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Outcome\n",
    "\n",
    "- A `BedrockReranker` object if reranking is enabled.\n",
    "- Otherwise, `reranker = None`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a20966",
   "metadata": {},
   "source": [
    "### Initialize reranker if a valid rerank model ID is provided in the configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38800c24-aa54-4d67-afcb-0fd5c41c4bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = BedrockReranker(exp_config_data.get(\"aws_region\"), exp_config_data.get(\"rerank_model_id\")) \\\n",
    "                if exp_config_data.get(\"rerank_model_id\").lower() != \"none\" \\\n",
    "                else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c335b9f7",
   "metadata": {},
   "source": [
    "### Load ground truth data in JSON reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af7e2cd3-41d7-42a6-a588-5f7d06a5fcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flotorch_core.storage.s3_storage:Reading data from S3 storage\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Traceback </span><span style=\"color: #ff7f7f; text-decoration-color: #ff7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">11</span>                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span><span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_chunk</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>) -&gt; Chunk:                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> Chunk(data=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.question)                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>11 questions_list = json_reader.read_as_model(gt_data_path, Question)                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/chethan/.local/lib/python3.10/site-packages/flotorch_core/reader/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">json_reader.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">33</span> in      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">read_as_model</span>                                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">:return: The model object.</span>                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">31 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">32 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span><span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">read_as_model</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, path: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>, model_class: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>) -&gt; List[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">object</span>]:                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>33 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>data = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.read(path)                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">34 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>                                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(data, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>):                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">36 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> [model_class(**item) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> item <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> data]                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/chethan/.local/lib/python3.10/site-packages/flotorch_core/reader/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">json_reader.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">read</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">:return: The JSON data.</span>                                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span><span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">read</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, path:<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>) -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">dict</span>:                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>23 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>data = <span style=\"color: #808000; text-decoration-color: #808000\">\"\"</span>.join(chunk.decode(<span style=\"color: #808000; text-decoration-color: #808000\">\"utf-8\"</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> chunk <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.storage_provider.read(pat    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> json.loads(data)                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span>                                                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/chethan/.local/lib/python3.10/site-packages/flotorch_core/reader/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">json_reader.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span> in      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;genexpr&gt;</span>                                                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">:return: The JSON data.</span>                                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span><span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">read</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, path:<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>) -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">dict</span>:                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>23 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>data = <span style=\"color: #808000; text-decoration-color: #808000\">\"\"</span>.join(chunk.decode(<span style=\"color: #808000; text-decoration-color: #808000\">\"utf-8\"</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> chunk <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.storage_provider.read(pat    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> json.loads(data)                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span>                                                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/chethan/.local/lib/python3.10/site-packages/flotorch_core/storage/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">s3_storage.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">59</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">read</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 56 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">Generator[bytes, None, None]: A generator that yields the data read from the</span>   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 57 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 58 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>logger.info(<span style=\"color: #808000; text-decoration-color: #808000\">'Reading data from S3 storage'</span>)                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span> 59 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._is_directory(path):                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 60 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">yield from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._read_directory(path)                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 61 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 62 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>response = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.s3_client.get_object(Bucket=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bucket, Key=path)             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/chethan/.local/lib/python3.10/site-packages/flotorch_core/storage/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">s3_storage.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">78</span> in      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_is_directory</span>                                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 75 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> path.endswith(<span style=\"color: #808000; text-decoration-color: #808000\">\"/\"</span>):  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Ensure the path ends with a slash to treat it as a</span>   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 76 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>path += <span style=\"color: #808000; text-decoration-color: #808000\">\"/\"</span>                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 77 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span> 78 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>response = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.s3_client.list_objects_v2(Bucket=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bucket, Prefix=path, MaxKe   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 79 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"Contents\"</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> response  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># If \"Contents\" exists, it's a directory</span>            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 80 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span>                                                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 81 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span><span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">_read_directory</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, path: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>) -&gt; Generator[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">bytes</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>]:                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/chethan/.local/lib/python3.10/site-packages/botocore/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">client.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">569</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_api_call</span>            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 566 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"{</span>py_operation_name<span style=\"color: #808000; text-decoration-color: #808000\">}() only accepts keyword arguments.\"</span>              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 567 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>)                                                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 568 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The \"self\" in this scope is referring to the BaseClient.</span>                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span> 569 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._make_api_call(operation_name, kwargs)                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 570 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 571 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>_api_call.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(py_operation_name)                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 572 </span>                                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/chethan/.local/lib/python3.10/site-packages/botocore/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">client.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1023</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_make_api_call</span>      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1020 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Code\"</span>                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1021 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>)                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1022 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>error_class = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.exceptions.from_code(error_code)                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>1023 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> error_class(parsed_response, operation_name)                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> parsed_response                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1026 </span>                                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ClientError: </span>An error occurred <span style=\"font-weight: bold\">(</span>ExpiredToken<span style=\"font-weight: bold\">)</span> when calling the ListObjectsV2 operation: The provided token has \n",
       "expired.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;0;0mâ•­â”€\u001b[0m\u001b[38;2;255;0;0mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0mTraceback \u001b[0m\u001b[1;2;38;2;255;0;0m(most recent call last)\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[38;2;255;0;0mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;255;0;0mâ”€â•®\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m11\u001b[0m                                                                                   \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2mâ”‚   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mget_chunk\u001b[0m(\u001b[96mself\u001b[0m) -> Chunk:                                                           \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m Chunk(data=\u001b[96mself\u001b[0m.question)                                                    \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m10 \u001b[0m                                                                                            \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m11 questions_list = json_reader.read_as_model(gt_data_path, Question)                          \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m12 \u001b[0m                                                                                            \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[2;33m/home/chethan/.local/lib/python3.10/site-packages/flotorch_core/reader/\u001b[0m\u001b[1;33mjson_reader.py\u001b[0m:\u001b[94m33\u001b[0m in      \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[92mread_as_model\u001b[0m                                                                                    \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m30 \u001b[0m\u001b[2;33mâ”‚   \u001b[0m\u001b[33m:return: The model object.\u001b[0m                                                              \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m31 \u001b[0m\u001b[2;33mâ”‚   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                     \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m32 \u001b[0m\u001b[2mâ”‚   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mread_as_model\u001b[0m(\u001b[96mself\u001b[0m, path: \u001b[96mstr\u001b[0m, model_class: \u001b[96mtype\u001b[0m) -> List[\u001b[96mobject\u001b[0m]:                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m33 \u001b[2mâ”‚   â”‚   \u001b[0mdata = \u001b[96mself\u001b[0m.read(path)                                                              \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m34 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m                                                                                    \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m35 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(data, \u001b[96mlist\u001b[0m):                                                          \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m36 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m [model_class(**item) \u001b[94mfor\u001b[0m item \u001b[95min\u001b[0m data]                                   \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[2;33m/home/chethan/.local/lib/python3.10/site-packages/flotorch_core/reader/\u001b[0m\u001b[1;33mjson_reader.py\u001b[0m:\u001b[94m23\u001b[0m in \u001b[92mread\u001b[0m \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m20 \u001b[0m\u001b[2;33mâ”‚   \u001b[0m\u001b[33m:return: The JSON data.\u001b[0m                                                                 \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m21 \u001b[0m\u001b[2;33mâ”‚   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                     \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2mâ”‚   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mread\u001b[0m(\u001b[96mself\u001b[0m, path:\u001b[96mstr\u001b[0m) -> \u001b[96mdict\u001b[0m:                                                       \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m23 \u001b[2mâ”‚   â”‚   \u001b[0mdata = \u001b[33m\"\u001b[0m\u001b[33m\"\u001b[0m.join(chunk.decode(\u001b[33m\"\u001b[0m\u001b[33mutf-8\u001b[0m\u001b[33m\"\u001b[0m) \u001b[94mfor\u001b[0m chunk \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.storage_provider.read(pat    \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m24 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m json.loads(data)                                                             \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m25 \u001b[0m\u001b[2mâ”‚   \u001b[0m                                                                                        \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m26 \u001b[0m\u001b[2;90mâ”‚   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                     \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[2;33m/home/chethan/.local/lib/python3.10/site-packages/flotorch_core/reader/\u001b[0m\u001b[1;33mjson_reader.py\u001b[0m:\u001b[94m23\u001b[0m in      \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[92m<genexpr>\u001b[0m                                                                                        \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m20 \u001b[0m\u001b[2;33mâ”‚   \u001b[0m\u001b[33m:return: The JSON data.\u001b[0m                                                                 \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m21 \u001b[0m\u001b[2;33mâ”‚   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                     \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2mâ”‚   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mread\u001b[0m(\u001b[96mself\u001b[0m, path:\u001b[96mstr\u001b[0m) -> \u001b[96mdict\u001b[0m:                                                       \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m23 \u001b[2mâ”‚   â”‚   \u001b[0mdata = \u001b[33m\"\u001b[0m\u001b[33m\"\u001b[0m.join(chunk.decode(\u001b[33m\"\u001b[0m\u001b[33mutf-8\u001b[0m\u001b[33m\"\u001b[0m) \u001b[94mfor\u001b[0m chunk \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.storage_provider.read(pat    \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m24 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m json.loads(data)                                                             \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m25 \u001b[0m\u001b[2mâ”‚   \u001b[0m                                                                                        \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m26 \u001b[0m\u001b[2;90mâ”‚   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                     \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[2;33m/home/chethan/.local/lib/python3.10/site-packages/flotorch_core/storage/\u001b[0m\u001b[1;33ms3_storage.py\u001b[0m:\u001b[94m59\u001b[0m in \u001b[92mread\u001b[0m \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 56 \u001b[0m\u001b[2;33mâ”‚   â”‚   â”‚   \u001b[0m\u001b[33mGenerator[bytes, None, None]: A generator that yields the data read from the\u001b[0m   \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 57 \u001b[0m\u001b[2;33mâ”‚   â”‚   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 58 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0mlogger.info(\u001b[33m'\u001b[0m\u001b[33mReading data from S3 storage\u001b[0m\u001b[33m'\u001b[0m)                                        \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m 59 \u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._is_directory(path):                                                       \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 60 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[94myield from\u001b[0m \u001b[96mself\u001b[0m._read_directory(path)                                          \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 61 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 62 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mresponse = \u001b[96mself\u001b[0m.s3_client.get_object(Bucket=\u001b[96mself\u001b[0m.bucket, Key=path)             \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[2;33m/home/chethan/.local/lib/python3.10/site-packages/flotorch_core/storage/\u001b[0m\u001b[1;33ms3_storage.py\u001b[0m:\u001b[94m78\u001b[0m in      \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[92m_is_directory\u001b[0m                                                                                    \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 75 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m path.endswith(\u001b[33m\"\u001b[0m\u001b[33m/\u001b[0m\u001b[33m\"\u001b[0m):  \u001b[2m# Ensure the path ends with a slash to treat it as a\u001b[0m   \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 76 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mpath += \u001b[33m\"\u001b[0m\u001b[33m/\u001b[0m\u001b[33m\"\u001b[0m                                                                    \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 77 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m                                                                                   \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m 78 \u001b[2mâ”‚   â”‚   \u001b[0mresponse = \u001b[96mself\u001b[0m.s3_client.list_objects_v2(Bucket=\u001b[96mself\u001b[0m.bucket, Prefix=path, MaxKe   \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 79 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mContents\u001b[0m\u001b[33m\"\u001b[0m \u001b[95min\u001b[0m response  \u001b[2m# If \"Contents\" exists, it's a directory\u001b[0m            \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 80 \u001b[0m\u001b[2mâ”‚   \u001b[0m                                                                                       \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 81 \u001b[0m\u001b[2mâ”‚   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92m_read_directory\u001b[0m(\u001b[96mself\u001b[0m, path: \u001b[96mstr\u001b[0m) -> Generator[\u001b[96mbytes\u001b[0m, \u001b[94mNone\u001b[0m, \u001b[94mNone\u001b[0m]:                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[2;33m/home/chethan/.local/lib/python3.10/site-packages/botocore/\u001b[0m\u001b[1;33mclient.py\u001b[0m:\u001b[94m569\u001b[0m in \u001b[92m_api_call\u001b[0m            \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 566 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mpy_operation_name\u001b[33m}\u001b[0m\u001b[33m() only accepts keyword arguments.\u001b[0m\u001b[33m\"\u001b[0m              \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 567 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0m)                                                                         \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 568 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[2m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m                    \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m 569 \u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._make_api_call(operation_name, kwargs)                            \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 570 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 571 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m_api_call.\u001b[91m__name__\u001b[0m = \u001b[96mstr\u001b[0m(py_operation_name)                                       \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m 572 \u001b[0m                                                                                          \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[2;33m/home/chethan/.local/lib/python3.10/site-packages/botocore/\u001b[0m\u001b[1;33mclient.py\u001b[0m:\u001b[94m1023\u001b[0m in \u001b[92m_make_api_call\u001b[0m      \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m1020 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mCode\u001b[0m\u001b[33m\"\u001b[0m                                                                    \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m1021 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m)                                                                             \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m1022 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0merror_class = \u001b[96mself\u001b[0m.exceptions.from_code(error_code)                           \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m1023 \u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[94mraise\u001b[0m error_class(parsed_response, operation_name)                            \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m1024 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m1025 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m parsed_response                                                        \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m1026 \u001b[0m                                                                                          \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
       "\u001b[1;91mClientError: \u001b[0mAn error occurred \u001b[1m(\u001b[0mExpiredToken\u001b[1m)\u001b[0m when calling the ListObjectsV2 operation: The provided token has \n",
       "expired.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Read ground truth json\n",
    "from pydantic import BaseModel\n",
    "from flotorch_core.chunking.chunking import Chunk\n",
    "class Question(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "    def get_chunk(self) -> Chunk:\n",
    "        return Chunk(data=self.question)\n",
    "\n",
    "questions_list = json_reader.read_as_model(gt_data_path, Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0992f6f5",
   "metadata": {},
   "source": [
    "### ğŸ¤– Perform vector search for each question chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e8c5c53-9308-4878-8cfc-378e718418c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hierarchical = exp_config_data.get(\"chunking_strategy\") == 'hierarchical'\n",
    "\n",
    "responses_list = []\n",
    "for question in questions_list:\n",
    "    question_chunk = question.get_chunk()\n",
    "    vector_response = vector_storage.search(question_chunk, int(exp_config_data.get(\"knn_num\")), hierarchical)\n",
    "    vector_response_result = vector_response.to_json()['result']\n",
    "    responses_list.append({'question':question, 'question_chunk':question_chunk, 'vector_response':vector_response, 'vector_response_result':vector_response_result, 'response_status':vector_response.status})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aba61c",
   "metadata": {},
   "source": [
    "### ğŸ” Rerank vector responses using the reranker if enabled and response is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a955930e-6e00-44e4-919d-6d6bb0953b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for each_response in responses_list:\n",
    "    response_status = each_response['response_status']\n",
    "    vector_response_result = each_response['vector_response_result']\n",
    "    if reranker and response_status:\n",
    "        vector_response = reranker.rerank_documents(each_response['question_chunk'].data, vector_response_result)\n",
    "        each_response['vector_response'] = vector_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38cd325",
   "metadata": {},
   "source": [
    "### ğŸ§  Generate answers and extract metadata for each response, applying guardrail checks if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c739274-0df4-4708-b5e4-400a088cb928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 16:50:23,511 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-21 16:50:25,337 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-21 16:50:26,213 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-21 16:50:26,822 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-21 16:50:27,603 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-21 16:50:28,428 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-21 16:50:29,335 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n"
     ]
    }
   ],
   "source": [
    "for each_response in responses_list:\n",
    "    response_status = each_response['response_status']\n",
    "    if response_status:\n",
    "        question = each_response['question']\n",
    "        vector_response = each_response['vector_response']\n",
    "        vector_response_result = each_response['vector_response_result']\n",
    "        metadata, answer = inferencer.generate_text(question.question, vector_response_result)\n",
    "        guardrail_blocked = metadata['guardrail_blocked'] if 'guardrail_blocked' in metadata else False\n",
    "        if guardrail_blocked:\n",
    "            answer_metadata = {}\n",
    "        else:\n",
    "            answer_metadata = metadata\n",
    "    else:\n",
    "        answer = metadata['guardrail_output']\n",
    "        metadata = {}\n",
    "        answer_metadata = {}\n",
    "        guardrail_blocked = vector_response.metadata['guardrail_blocked'] if 'guardrail_blocked' in vector_response.metadata else False\n",
    "    each_response['metadata'] = metadata\n",
    "    each_response['answer'] = answer\n",
    "    each_response['answer_metadata'] = answer_metadata\n",
    "    each_response['guardrail_blocked'] = guardrail_blocked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932195d0",
   "metadata": {},
   "source": [
    "### ğŸ“¦ Aggregate final results with question, answer, guardrail assessments, and reference context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3eeaedf-be15-4f47-9ac1-af1023ea8d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n',\n",
       "  'answer': \"To facilitate a smooth onboarding experience with Amazon Bedrock IDE in Amazon SageMaker Unified Studio, you can find detailed documentation on the Amazon Bedrock IDE User Guide. If you have any additional questions or need further assistance, please don't hesitate to reach out to your AWS account team.\",\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 313,\n",
       "   'outputTokens': 57,\n",
       "   'totalTokens': 370,\n",
       "   'latencyMs': 779},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n',\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List\\n\\n\\ndef separate_paren_groups(paren_string: str) -> List[str]:\\n    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\\n    separate those group into separate strings and return the list of those.\\n    Separate groups are balanced (each open brace is properly closed) and not nested within each other\\n    Ignore any spaces in the input string.\\n    >>> separate_paren_groups(\\'( ) (( )) (( )( ))\\')\\n    [\\'()\\', \\'(())\\', \\'(()())\\']\\n    \"\"\"\\n',\n",
       "  'answer': 'Sorry, I don\\'t have sufficient information to provide an answer. \\n\\nHere\\'s the implementation of the `separate_paren_groups` function as described:\\n\\n```python\\nfrom typing import List\\n\\ndef separate_paren_groups(paren_string: str) -> List[str]:\\n    \"\"\" \\n    Input to this function is a string containing multiple groups of nested parentheses. \\n    Your goal is to separate those groups into separate strings and return the list of those.\\n    Separate groups are balanced (each open brace is properly closed) and not nested within each other.\\n    Ignore any spaces in the input string.\\n    \\n    >>> separate_paren_groups(\\'( ) (( )) (( )( ))\\')\\n    [\\'()\\', \\'(())\\', \\'(()())\\']\\n    \"\"\"\\n    paren_string = paren_string.replace(\" \", \"\")  # Remove spaces\\n    groups = []\\n    current_group = \"\"\\n    balance = 0\\n    \\n    for char in paren_string:\\n        if char == \\'(\\':\\n            balance += 1\\n        elif char == \\')\\':\\n            balance -= 1\\n        \\n        current_group += char\\n        \\n        if balance == 0:\\n            groups.append(current_group)\\n            current_group = \"\"\\n    \\n    return groups\\n\\n# Example usage:\\nprint(separate_paren_groups(\\'( ) (( )) (( )( ))\\'))  # Output: [\\'()\\', \\'(())\\', \\'(()())\\']\\n```',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 300,\n",
       "   'outputTokens': 298,\n",
       "   'totalTokens': 598,\n",
       "   'latencyMs': 3326},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': \"    result = []\\n    current_string = []\\n    current_depth = 0\\n\\n    for c in paren_string:\\n        if c == '(':\\n            current_depth += 1\\n            current_string.append(c)\\n        elif c == ')':\\n            current_depth -= 1\\n            current_string.append(c)\\n\\n            if current_depth == 0:\\n                result.append(''.join(current_string))\\n                current_string.clear()\\n\\n    return result\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': '\\n\\ndef truncate_number(number: float) -> float:\\n    \"\"\" Given a positive floating point number, it can be decomposed into\\n    and integer part (largest integer smaller than given number) and decimals\\n    (leftover part always smaller than 1).\\n\\n    Return the decimal part of the number.\\n    >>> truncate_number(3.5)\\n    0.5\\n    \"\"\"\\n',\n",
       "  'answer': \"Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers\",\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 281,\n",
       "   'outputTokens': 25,\n",
       "   'totalTokens': 306,\n",
       "   'latencyMs': 454},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    return number % 1.0\\n',\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List\\n\\n\\ndef below_zero(operations: List[int]) -> bool:\\n    \"\"\" You\\'re given a list of deposit and withdrawal operations on a bank account that starts with\\n    zero balance. Your task is to detect if at any point the balance of account fallls below zero, and\\n    at that point function should return True. Otherwise it should return False.\\n    >>> below_zero([1, 2, 3])\\n    False\\n    >>> below_zero([1, 2, -4, 5])\\n    True\\n    \"\"\"\\n',\n",
       "  'answer': \"Sorry, I don't have sufficient information to provide an answer.\",\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 320,\n",
       "   'outputTokens': 14,\n",
       "   'totalTokens': 334,\n",
       "   'latencyMs': 344},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    balance = 0\\n\\n    for op in operations:\\n        balance += op\\n        if balance < 0:\\n            return True\\n\\n    return False\\n',\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List\\n\\n\\ndef mean_absolute_deviation(numbers: List[float]) -> float:\\n    \"\"\" For a given list of input numbers, calculate Mean Absolute Deviation\\n    around the mean of this dataset.\\n    Mean Absolute Deviation is the average absolute difference between each\\n    element and a centerpoint (mean in this case):\\n    MAD = average | x - x_mean |\\n    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])\\n    1.0\\n    \"\"\"\\n',\n",
       "  'answer': \"Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers\",\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 318,\n",
       "   'outputTokens': 25,\n",
       "   'totalTokens': 343,\n",
       "   'latencyMs': 418},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    mean = sum(numbers) / len(numbers)\\n    return sum(abs(x - mean) for x in numbers) / len(numbers)\\n',\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List\\n\\n\\ndef intersperse(numbers: List[int], delimeter: int) -> List[int]:\\n    \"\"\" Insert a number \\'delimeter\\' between every two consecutive elements of input list `numbers\\'\\n    >>> intersperse([], 4)\\n    []\\n    >>> intersperse([1, 2, 3], 4)\\n    [1, 4, 2, 4, 3]\\n    \"\"\"\\n',\n",
       "  'answer': 'Sorry, I don\\'t have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers\\n\\nHere\\'s the implementation of the `intersperse` function as requested:\\n\\n```python\\nfrom typing import List\\n\\ndef intersperse(numbers: List[int], delimiter: int) -> List[int]:\\n    \"\"\" Insert a number \\'delimiter\\' between every two consecutive elements of input list `numbers\\'\\n    >>> intersperse([], 4)\\n    []\\n    >>> intersperse([1, 2, 3], 4)\\n    [1, 4, 2, 4, 3]\\n    \"\"\"\\n    if not numbers:\\n        return []\\n    \\n    result = [numbers[0]]\\n    for num in numbers[1:]:\\n        result.append(delimiter)\\n        result.append(num)\\n    \\n    return result\\n```',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 262,\n",
       "   'outputTokens': 189,\n",
       "   'totalTokens': 451,\n",
       "   'latencyMs': 2104},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    if not numbers:\\n        return []\\n\\n    result = []\\n\\n    for n in numbers[:-1]:\\n        result.append(n)\\n        result.append(delimeter)\\n\\n    result.append(numbers[-1])\\n\\n    return result\\n',\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List\\n\\n\\ndef parse_nested_parens(paren_string: str) -> List[int]:\\n    \"\"\" Input to this function is a string represented multiple groups for nested parentheses separated by spaces.\\n    For each of the group, output the deepest level of nesting of parentheses.\\n    E.g. (()()) has maximum two levels of nesting while ((())) has three.\\n\\n    >>> parse_nested_parens(\\'(()()) ((())) () ((())()())\\')\\n    [2, 3, 1, 3]\\n    \"\"\"\\n',\n",
       "  'answer': 'Sorry, I don\\'t have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers.\\n\\nHere is the implementation of the `parse_nested_parens` function as requested:\\n\\n```python\\nfrom typing import List\\n\\ndef parse_nested_parens(paren_string: str) -> List[int]:\\n    \"\"\" Input to this function is a string represented multiple groups for nested parentheses separated by spaces.\\n    For each of the group, output the deepest level of nesting of parentheses.\\n    E.g. (()()) has maximum two levels of nesting while ((())) has three.\\n\\n    >>> parse_nested_parens(\\'(()()) ((())) () ((())()())\\')\\n    [2, 3, 1, 3]\\n    \"\"\"\\n    def max_depth(s: str) -> int:\\n        depth = 0\\n        max_depth = 0\\n        for char in s:\\n            if char == \\'(\\':\\n                depth += 1\\n                if depth > max_depth:\\n                    max_depth = depth\\n            elif char == \\')\\':\\n                depth -= 1\\n        return max_depth\\n    \\n    groups = paren_string.split()\\n    depths = [max_depth(group) for group in groups]\\n    return depths\\n\\n# Example usage:\\nprint(parse_nested_parens(\\'(()()) ((())) () ((())()())\\'))  # Output: [2, 3, 1, 3]\\n```',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 313,\n",
       "   'outputTokens': 313,\n",
       "   'totalTokens': 626,\n",
       "   'latencyMs': 3357},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': \"    def parse_paren_group(s):\\n        depth = 0\\n        max_depth = 0\\n        for c in s:\\n            if c == '(':\\n                depth += 1\\n                max_depth = max(depth, max_depth)\\n            else:\\n                depth -= 1\\n\\n        return max_depth\\n\\n    return [parse_paren_group(x) for x in paren_string.split(' ') if x]\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List\\n\\n\\ndef filter_by_substring(strings: List[str], substring: str) -> List[str]:\\n    \"\"\" Filter an input list of strings only for ones that contain given substring\\n    >>> filter_by_substring([], \\'a\\')\\n    []\\n    >>> filter_by_substring([\\'abc\\', \\'bacd\\', \\'cde\\', \\'array\\'], \\'a\\')\\n    [\\'abc\\', \\'bacd\\', \\'array\\']\\n    \"\"\"\\n',\n",
       "  'answer': \"Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers\",\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 301,\n",
       "   'outputTokens': 25,\n",
       "   'totalTokens': 326,\n",
       "   'latencyMs': 414},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    return [x for x in strings if substring in x]\\n',\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List, Tuple\\n\\n\\ndef sum_product(numbers: List[int]) -> Tuple[int, int]:\\n    \"\"\" For a given list of integers, return a tuple consisting of a sum and a product of all the integers in a list.\\n    Empty sum should be equal to 0 and empty product should be equal to 1.\\n    >>> sum_product([])\\n    (0, 1)\\n    >>> sum_product([1, 2, 3, 4])\\n    (10, 24)\\n    \"\"\"\\n',\n",
       "  'answer': 'Amazon Bedrock IDE itself is free to use, but users are charged for the underlying resources utilized by their generative AI applications. These resources include the associated models, Guardrails, and Knowledge Bases used within the applications. For detailed pricing information, please visit the [Amazon Bedrock pricing page](https://aws.amazon.com/bedrock/pricing/).',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 315,\n",
       "   'outputTokens': 75,\n",
       "   'totalTokens': 390,\n",
       "   'latencyMs': 904},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    sum_value = 0\\n    prod_value = 1\\n\\n    for n in numbers:\\n        sum_value += n\\n        prod_value *= n\\n    return sum_value, prod_value\\n',\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List, Tuple\\n\\n\\ndef rolling_max(numbers: List[int]) -> List[int]:\\n    \"\"\" From a given list of integers, generate a list of rolling maximum element found until given moment\\n    in the sequence.\\n    >>> rolling_max([1, 2, 3, 2, 3, 4, 2])\\n    [1, 2, 3, 3, 3, 4, 4]\\n    \"\"\"\\n',\n",
       "  'answer': \"Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers\",\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 266,\n",
       "   'outputTokens': 25,\n",
       "   'totalTokens': 291,\n",
       "   'latencyMs': 384},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    running_max = None\\n    result = []\\n\\n    for n in numbers:\\n        if running_max is None:\\n            running_max = n\\n        else:\\n            running_max = max(running_max, n)\\n\\n        result.append(running_max)\\n\\n    return result\\n',\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': '\\n\\ndef is_palindrome(string: str) -> bool:\\n    \"\"\" Test if given string is a palindrome \"\"\"\\n    return string == string[::-1]\\n\\n\\ndef make_palindrome(string: str) -> str:\\n    \"\"\" Find the shortest palindrome that begins with a supplied string.\\n    Algorithm idea is simple:\\n    - Find the longest postfix of supplied string that is a palindrome.\\n    - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix.\\n    >>> make_palindrome(\\'\\')\\n    \\'\\'\\n    >>> make_palindrome(\\'cat\\')\\n    \\'catac\\'\\n    >>> make_palindrome(\\'cata\\')\\n    \\'catac\\'\\n    \"\"\"\\n',\n",
       "  'answer': \"Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers\",\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 352,\n",
       "   'outputTokens': 25,\n",
       "   'totalTokens': 377,\n",
       "   'latencyMs': 458},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': \"    if not string:\\n        return ''\\n\\n    beginning_of_suffix = 0\\n\\n    while not is_palindrome(string[beginning_of_suffix:]):\\n        beginning_of_suffix += 1\\n\\n    return string + string[:beginning_of_suffix][::-1]\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List\\n\\n\\ndef string_xor(a: str, b: str) -> str:\\n    \"\"\" Input are two strings a and b consisting only of 1s and 0s.\\n    Perform binary XOR on these inputs and return result also as a string.\\n    >>> string_xor(\\'010\\', \\'110\\')\\n    \\'100\\'\\n    \"\"\"\\n',\n",
       "  'answer': 'Sorry, I don\\'t have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers\\n\\nRegarding the `string_xor` function, here\\'s the implementation:\\n\\n```python\\ndef string_xor(a: str, b: str) -> str:\\n    \"\"\" Input are two strings a and b consisting only of 1s and 0s.\\n    Perform binary XOR on these inputs and return result also as a string.\\n    >>> string_xor(\\'010\\', \\'110\\')\\n    \\'100\\'\\n    \"\"\"\\n    result = []\\n    for bit_a, bit_b in zip(a, b):\\n        if bit_a == bit_b:\\n            result.append(\\'0\\')\\n        else:\\n            result.append(\\'1\\')\\n    return \\'\\'.join(result)\\n```',\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 247,\n",
       "   'outputTokens': 180,\n",
       "   'totalTokens': 427,\n",
       "   'latencyMs': 2067},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': \"    def xor(i, j):\\n        if i == j:\\n            return '0'\\n        else:\\n            return '1'\\n\\n    return ''.join(xor(x, y) for x, y in zip(a, b))\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List, Optional\\n\\n\\ndef longest(strings: List[str]) -> Optional[str]:\\n    \"\"\" Out of list of strings, return the longest one. Return the first one in case of multiple\\n    strings of the same length. Return None in case the input list is empty.\\n    >>> longest([])\\n\\n    >>> longest([\\'a\\', \\'b\\', \\'c\\'])\\n    \\'a\\'\\n    >>> longest([\\'a\\', \\'bb\\', \\'ccc\\'])\\n    \\'ccc\\'\\n    \"\"\"\\n',\n",
       "  'answer': \"Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers\",\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 303,\n",
       "   'outputTokens': 25,\n",
       "   'totalTokens': 328,\n",
       "   'latencyMs': 384},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    if not strings:\\n        return None\\n\\n    maxlen = max(len(x) for x in strings)\\n    for s in strings:\\n        if len(s) == maxlen:\\n            return s\\n',\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': '\\n\\ndef greatest_common_divisor(a: int, b: int) -> int:\\n    \"\"\" Return a greatest common divisor of two integers a and b\\n    >>> greatest_common_divisor(3, 5)\\n    1\\n    >>> greatest_common_divisor(25, 15)\\n    5\\n    \"\"\"\\n',\n",
       "  'answer': \"Sorry, I don't have sufficient information to provide an answer.\",\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 253,\n",
       "   'outputTokens': 14,\n",
       "   'totalTokens': 267,\n",
       "   'latencyMs': 318},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    while b:\\n        a, b = b, a % b\\n    return a\\n',\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List\\n\\n\\ndef all_prefixes(string: str) -> List[str]:\\n    \"\"\" Return list of all prefixes from shortest to longest of the input string\\n    >>> all_prefixes(\\'abc\\')\\n    [\\'a\\', \\'ab\\', \\'abc\\']\\n    \"\"\"\\n',\n",
       "  'answer': \"Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers\",\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 224,\n",
       "   'outputTokens': 25,\n",
       "   'totalTokens': 249,\n",
       "   'latencyMs': 465},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    result = []\\n\\n    for i in range(len(string)):\\n        result.append(string[:i+1])\\n    return result\\n',\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': '\\n\\ndef string_sequence(n: int) -> str:\\n    \"\"\" Return a string containing space-delimited numbers starting from 0 upto n inclusive.\\n    >>> string_sequence(0)\\n    \\'0\\'\\n    >>> string_sequence(5)\\n    \\'0 1 2 3 4 5\\'\\n    \"\"\"\\n',\n",
       "  'answer': \"Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers\",\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 267,\n",
       "   'outputTokens': 25,\n",
       "   'totalTokens': 292,\n",
       "   'latencyMs': 403},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': \"    return ' '.join([str(x) for x in range(n + 1)])\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': '\\n\\ndef count_distinct_characters(string: str) -> int:\\n    \"\"\" Given a string, find out how many distinct characters (regardless of case) does it consist of\\n    >>> count_distinct_characters(\\'xyzXYZ\\')\\n    3\\n    >>> count_distinct_characters(\\'Jerry\\')\\n    4\\n    \"\"\"\\n',\n",
       "  'answer': \"Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers\",\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 267,\n",
       "   'outputTokens': 25,\n",
       "   'totalTokens': 292,\n",
       "   'latencyMs': 404},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    return len(set(string.lower()))\\n',\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List\\n\\n\\ndef parse_music(music_string: str) -> List[int]:\\n    \"\"\" Input to this function is a string representing musical notes in a special ASCII format.\\n    Your task is to parse this string and return list of integers corresponding to how many beats does each\\n    not last.\\n\\n    Here is a legend:\\n    \\'o\\' - whole note, lasts four beats\\n    \\'o|\\' - half note, lasts two beats\\n    \\'.|\\' - quater note, lasts one beat\\n\\n    >>> parse_music(\\'o o| .| o| o| .| .| .| .| o o\\')\\n    [4, 2, 1, 2, 2, 1, 1, 1, 1, 4, 4]\\n    \"\"\"\\n',\n",
       "  'answer': \"Sorry, I don't have sufficient information to provide an answer.\",\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 372,\n",
       "   'outputTokens': 14,\n",
       "   'totalTokens': 386,\n",
       "   'latencyMs': 321},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': \"    note_map = {'o': 4, 'o|': 2, '.|': 1}\\n    return [note_map[x] for x in music_string.split(' ') if x]\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': '\\n\\ndef how_many_times(string: str, substring: str) -> int:\\n    \"\"\" Find how many times a given substring can be found in the original string. Count overlaping cases.\\n    >>> how_many_times(\\'\\', \\'a\\')\\n    0\\n    >>> how_many_times(\\'aaa\\', \\'a\\')\\n    3\\n    >>> how_many_times(\\'aaaa\\', \\'aa\\')\\n    3\\n    \"\"\"\\n',\n",
       "  'answer': \"To facilitate a smooth onboarding experience with Amazon Bedrock IDE in Amazon SageMaker Unified Studio, you can find detailed documentation on the Amazon Bedrock IDE User Guide. If you have any additional questions or need further assistance, please don't hesitate to reach out to your AWS account team.\",\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 274,\n",
       "   'outputTokens': 57,\n",
       "   'totalTokens': 331,\n",
       "   'latencyMs': 725},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': '    times = 0\\n\\n    for i in range(len(string) - len(substring) + 1):\\n        if string[i:i+len(substring)] == substring:\\n            times += 1\\n\\n    return times\\n',\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}},\n",
       " {'question': 'from typing import List\\n\\n\\ndef sort_numbers(numbers: str) -> str:\\n    \"\"\" Input is a space-delimited string of numberals from \\'zero\\' to \\'nine\\'.\\n    Valid choices are \\'zero\\', \\'one\\', \\'two\\', \\'three\\', \\'four\\', \\'five\\', \\'six\\', \\'seven\\', \\'eight\\' and \\'nine\\'.\\n    Return the string with numbers sorted from smallest to largest\\n    >>> sort_numbers(\\'three one five\\')\\n    \\'one three five\\'\\n    \"\"\"\\n',\n",
       "  'answer': \"Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers\",\n",
       "  'guardrails_output_assessment': None,\n",
       "  'guardrails_context_assessment': None,\n",
       "  'guardrails_input_assessment': None,\n",
       "  'guardrails_blocked': False,\n",
       "  'guardrails_block_level': '',\n",
       "  'answer_metadata': {'inputTokens': 310,\n",
       "   'outputTokens': 25,\n",
       "   'totalTokens': 335,\n",
       "   'latencyMs': 432},\n",
       "  'reference_contexts': [],\n",
       "  'gt_answer': \"    value_map = {\\n        'zero': 0,\\n        'one': 1,\\n        'two': 2,\\n        'three': 3,\\n        'four': 4,\\n        'five': 5,\\n        'six': 6,\\n        'seven': 7,\\n        'eight': 8,\\n        'nine': 9\\n    }\\n    return ' '.join(sorted([x for x in numbers.split(' ') if x], key=lambda x: value_map[x]))\\n\",\n",
       "  'query_metadata': {'input_token': 0, 'latency_ms': 0}}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for each_response in responses_list:\n",
    "    metadata = each_response['metadata']\n",
    "    vector_response = each_response['vector_response']\n",
    "    vector_response_result = each_response['vector_response_result']\n",
    "    result.append(\n",
    "                {'question':each_response['question'].question,\n",
    "                'answer':each_response['answer'],\n",
    "                'guardrails_output_assessment':metadata['guardrail_output_assessment'] if 'guardrail_output_assessment' in metadata else None,\n",
    "                'guardrails_context_assessment':vector_response.metadata['guardrail_context_assessment'] if 'guardrail_context_assessment' in vector_response.metadata else None,\n",
    "                'guardrails_input_assessment':vector_response.metadata['guardrail_input_assessment'] if 'guardrail_input_assessment' in vector_response.metadata else None,\n",
    "                'guardrails_blocked':each_response['guardrail_blocked'],\n",
    "                'guardrails_block_level':vector_response.metadata['block_level'] if 'block_level' in vector_response.metadata else \"\",\n",
    "                'answer_metadata':each_response['answer_metadata'],\n",
    "                'reference_contexts':[res['text'] for res in vector_response_result] if vector_response_result else [],\n",
    "                'gt_answer':each_response['question'].answer,\n",
    "                'query_metadata':vector_response.metadata['embedding_metadata'].to_json() if 'embedding_metadata' in vector_response.metadata else None\n",
    "                })\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba6ebdd-2388-4e1e-8681-41de9fedeab4",
   "metadata": {},
   "source": [
    "### ğŸ“¦ Calculate Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9857bc86-5e08-4d26-90a6-cc92bf07f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.cost_calculation import calculate_total_cost\n",
    "total_cost, results = calculate_total_cost(exp_config_data, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84674286",
   "metadata": {},
   "source": [
    "### ğŸ’¾ Save the aggregated results to a JSON file for inference metrics\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e75a82ea-3e51-4a58-bdb1-025a7f6b0af4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dc91fa0-5952-4d83-8372-ed46d47dd91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./results/{exp_config_data['retrieval_service']}_inference_metrics.json\", \"w\") as json_file:\n",
    "    json.dump(results, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f72648-1282-4fe0-b78b-6b9a1bd7dcad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01458e5e-7103-4c68-957a-c67e2213e20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a903173-61d3-4bca-b423-cf92d3c0e5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2fa652-dd59-4f5e-9b73-2af0b79020d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
