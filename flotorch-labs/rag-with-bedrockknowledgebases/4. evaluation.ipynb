{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a6fdaec-a86e-47f7-9ed9-3d0242e5a9b3",
   "metadata": {},
   "source": [
    "# 📊 Evaluation in Flotorch\n",
    "\n",
    "[Flotorch](https://www.flotorch.ai/) provides a comprehensive evaluation framework for Retrieval-Augmented Generation (RAG) systems. It helps assess and compare Large Language Models (LLMs) based on relevance, quality, cost, and performance to support enterprise-grade deployments.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Key Evaluation Features\n",
    "\n",
    "- **Automated LLM Evaluation**  \n",
    "  Flotorch automates evaluation across:\n",
    "  - Relevance\n",
    "  - Fluency\n",
    "  - Robustness\n",
    "  - Cost\n",
    "  - Execution Speed\n",
    "\n",
    "- **Performance Metrics**  \n",
    "  It generates quantitative scores for evaluating how well a model performs across different criteria.\n",
    "\n",
    "- **Cost and Time Insights**  \n",
    "  Offers pricing and latency breakdowns for different LLM setups, enabling cost-effective choices.\n",
    "\n",
    "- **Data-Driven Decision-Making**  \n",
    "  Helps teams align LLM usage with specific application goals, budget, and performance needs.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Evaluation Workflow\n",
    "\n",
    "1. **Experiment Configuration**  \n",
    "   Define models, parameters, and goals for evaluation.\n",
    "\n",
    "2. **Automated Execution**  \n",
    "   Run evaluation pipelines to generate performance data.\n",
    "\n",
    "3. **Results Analysis**  \n",
    "   View dashboards or reports that summarize evaluation results.\n",
    "\n",
    "4. **Expert Evaluation (Optional)**  \n",
    "   Combine automatic evaluation with human review for more nuanced feedback.\n",
    "\n",
    "---\n",
    "\n",
    "This evaluation framework enables continuous monitoring, benchmarking, and optimization of RAG systems using LLMs, helping organizations deploy more reliable and efficient AI solutions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e067c92-f0f4-4135-a351-bf1f7c10d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "prompt_file_path = './data/eval_prompt.json'\n",
    "with open(prompt_file_path, 'r') as f:\n",
    "    prompt = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce74afb-8660-4576-b446-2b73f4b38191",
   "metadata": {},
   "source": [
    "## Load experiment config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7bb7517-ea81-426b-8d2f-2e246970b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config_data = {\n",
    "            \"temp_retrieval_llm\": 0.1,\n",
    "            \"retrival_service\": \"bedrock\",\n",
    "            \"eval_retrieval_model\": \"cohere.command-r-v1:0\",\n",
    "            \"eval_prompt\": prompt,\n",
    "            \"aws_region\":\"us-east-1\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54042c51-bdab-4524-922c-d65651d09004",
   "metadata": {},
   "source": [
    "## Load inference metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982fb77b-f992-4cf9-ba5f-6e8e63941a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f\"./results/{exp_config_data['retrival_service']}_inference_metrics.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59153913-5f0a-4a5f-8fd6-a7af3f5ae19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_prompt': 'Assume you are a human expert in grading Python function implementations.You are given a function definition, a ground truth implementation, a model prediction, and example test cases provided in the test field. Judge if the model\\'s implementation matches the ground truth by following these steps:\\n\\n1. Assume the Ground Truth is always correct.\\n2. If the Prediction is incomplete or shows it does not attempt a real solution, set \"score\" to 0.\\n3. If the Prediction exactly matches the Ground Truth implementation, set \"score\" to 1.\\n4. If the Prediction does not exactly match the Ground Truth, compare the functional correctness:\\n  - Run the example test cases provided in the test field mentally or by inspection.\\n  - If all test cases would still pass with the Prediction code, set \"score\" to 1.\\n  - If any test case would fail, set \"score\" to 0.\\n5. If the prediction contains syntax errors or logic unrelated to the task, set \"score\" to 0.\\n6. If the prediction solves the task in a different but correct and efficient way, set \"score\" to 1.\\n\\n### Output a JSON blob with an \"explanation\" field explaining your judgment concisely, and a \"score\" field with value 1 or 0.\\nYou should make the judgment based on provided examples.\\n\\n### Examples:\\n\\nQuestion: \"def string_sequence(n: int) -> str:\\\\n    \\\\\"\\\\\"\\\\\" Return a string containing space-delimited numbers starting from 0 upto n inclusive.\\\\n    \\\\\"\\\\\"\\\\\"\"\\nGround Truth:\\n    return \\' \\'.join([str(x) for x in range(n + 1)])\\nPrediction:\\n    return \\' \\'.join(str(i) for i in range(n + 1))\\nTest: [\\'string_sequence(0) == \"0\"\\', \\'string_sequence(5) == \"0 1 2 3 4 5\"\\']\\nOutput: {{ \"score\": 1, \"explanation\": \"Prediction is functionally equivalent and passes all test cases.\"}}\\n\\nQuestion: \"def all_prefixes(string: str) -> List[str]:\\\\n    \\\\\"\\\\\"\\\\\" Return list of all prefixes from shortest to longest of the input string\\\\n    \\\\\"\\\\\"\\\\\"\"\\nGround Truth:\\n    result = []\\n    for i in range(len(string)):\\n        result.append(string[:i+1])\\n    return result\\nPrediction:\\n    return [string[i:] for i in range(len(string))]\\nTest: [\"all_prefixes(\\'abc\\') == [\\'a\\', \\'ab\\', \\'abc\\']\"]\\nOutput: {{ \"score\": 0, \"explanation\": \"Prediction generates suffixes instead of prefixes, so test fails.\"}}\\n\\nQuestion: {user_query}\\nGround Truth:\\n{ground_truth}\\nPrediction:\\n{prediction}\\nTest:\\n{test}'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c847c-dec5-4bb0-a29f-1f74f2ee5d91",
   "metadata": {},
   "source": [
    "## Load Evaluator Class\n",
    "\n",
    "### 🧠 Evaluation with `CustomEvaluator`\n",
    "\n",
    "```python\n",
    "processor = CustomEvaluator(evaluator_llm=exp_config_data['eval_retrieval_model'])\n",
    "results = processor.evaluate(data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 Step-by-Step Breakdown\n",
    "\n",
    "| Line | Description |\n",
    "|------|-------------|\n",
    "| `processor = CustomEvaluator(...)` | Instantiates a `CustomEvaluator` using a language model specified in the config (`exp_config_data['eval_retrieval_model']`). |\n",
    "| `results = processor.evaluate(data)` | Runs the evaluation on the `data` using the evaluator, returning performance metrics or scoring output. |\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧩 Key Components\n",
    "\n",
    "- **`CustomEvaluator`**: A custom class designed to handle evaluation logic, potentially wrapping RAGAS or similar frameworks.\n",
    "- **`evaluator_llm`**: The evaluation language model (e.g. GPT, Claude, etc.) used for scoring responses.\n",
    "- **`data`**: A list of evaluation items (e.g. questions, answers, reference contexts).\n",
    "- **`results`**: The output from the evaluation — typically a dictionary or structured result with metric scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "913229fb-86b2-4da2-9321-a461618a5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluator import CustomEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d81ee62-cff0-4581-9274-ba047fe199b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|                                                                                                                                            | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'reference_contexts', 'gt_answer', 'test', 'query_metadata', 'inference_cost'])\n",
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n",
      "    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\n",
      "    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\n",
      "    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\n",
      "    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\n",
      "    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\n",
      "    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\n",
      "\n",
      "\n",
      "cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   5%|██████▌                                                                                                                             | 1/20 [00:01<00:37,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'reference_contexts', 'gt_answer', 'test', 'query_metadata', 'inference_cost'])\n",
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate('(()()) ((())) () ((())()())') == [\n",
      "        '(()())', '((()))', '()', '((())()())'\n",
      "    ]\n",
      "    assert candidate('() (()) ((())) (((())))') == [\n",
      "        '()', '(())', '((()))', '(((())))'\n",
      "    ]\n",
      "    assert candidate('(()(())((())))') == [\n",
      "        '(()(())((())))'\n",
      "    ]\n",
      "    assert candidate('( ) (( )) (( )( ))') == ['()', '(())', '(()())']\n",
      "\n",
      "cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  10%|█████████████▏                                                                                                                      | 2/20 [00:03<00:26,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'reference_contexts', 'gt_answer', 'test', 'query_metadata', 'inference_cost'])\n",
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate(3.5) == 0.5\n",
      "    assert abs(candidate(1.33) - 0.33) < 1e-6\n",
      "    assert abs(candidate(123.456) - 0.456) < 1e-6\n",
      "\n",
      "cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  15%|███████████████████▊                                                                                                                | 3/20 [00:03<00:20,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'reference_contexts', 'gt_answer', 'test', 'query_metadata', 'inference_cost'])\n",
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate([]) == False\n",
      "    assert candidate([1, 2, -3, 1, 2, -3]) == False\n",
      "    assert candidate([1, 2, -4, 5, 6]) == True\n",
      "    assert candidate([1, -1, 2, -2, 5, -5, 4, -4]) == False\n",
      "    assert candidate([1, -1, 2, -2, 5, -5, 4, -5]) == True\n",
      "    assert candidate([1, -2, 2, -2, 5, -5, 4, -4]) == True\n",
      "\n",
      "cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  20%|██████████████████████████▍                                                                                                         | 4/20 [00:05<00:18,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'reference_contexts', 'gt_answer', 'test', 'query_metadata', 'inference_cost'])\n",
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert abs(candidate([1.0, 2.0, 3.0]) - 2.0/3.0) < 1e-6\n",
      "    assert abs(candidate([1.0, 2.0, 3.0, 4.0]) - 1.0) < 1e-6\n",
      "    assert abs(candidate([1.0, 2.0, 3.0, 4.0, 5.0]) - 6.0/5.0) < 1e-6\n",
      "\n",
      "\n",
      "cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  25%|█████████████████████████████████                                                                                                   | 5/20 [00:06<00:17,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'reference_contexts', 'gt_answer', 'test', 'query_metadata', 'inference_cost'])\n",
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate([], 7) == []\n",
      "    assert candidate([5, 6, 3, 2], 8) == [5, 8, 6, 8, 3, 8, 2]\n",
      "    assert candidate([2, 2, 2], 2) == [2, 2, 2, 2, 2]\n",
      "\n",
      "cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  30%|███████████████████████████████████████▌                                                                                            | 6/20 [00:07<00:15,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'reference_contexts', 'gt_answer', 'test', 'query_metadata', 'inference_cost'])\n",
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate('(()()) ((())) () ((())()())') == [2, 3, 1, 3]\n",
      "    assert candidate('() (()) ((())) (((())))') == [1, 2, 3, 4]\n",
      "    assert candidate('(()(())((())))') == [4]\n",
      "\n",
      "cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  35%|██████████████████████████████████████████████▏                                                                                     | 7/20 [00:08<00:13,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'reference_contexts', 'gt_answer', 'test', 'query_metadata', 'inference_cost'])\n",
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate([], 'john') == []\n",
      "    assert candidate(['xxx', 'asd', 'xxy', 'john doe', 'xxxAAA', 'xxx'], 'xxx') == ['xxx', 'xxxAAA', 'xxx']\n",
      "    assert candidate(['xxx', 'asd', 'aaaxxy', 'john doe', 'xxxAAA', 'xxx'], 'xx') == ['xxx', 'aaaxxy', 'xxxAAA', 'xxx']\n",
      "    assert candidate(['grunt', 'trumpet', 'prune', 'gruesome'], 'run') == ['grunt', 'prune']\n",
      "\n",
      "cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  40%|████████████████████████████████████████████████████▊                                                                               | 8/20 [00:09<00:11,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'reference_contexts', 'gt_answer', 'test', 'query_metadata', 'inference_cost'])\n",
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate([]) == (0, 1)\n",
      "    assert candidate([1, 1, 1]) == (3, 1)\n",
      "    assert candidate([100, 0]) == (100, 0)\n",
      "    assert candidate([3, 5, 7]) == (3 + 5 + 7, 3 * 5 * 7)\n",
      "    assert candidate([10]) == (10, 10)\n",
      "\n",
      "cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  45%|███████████████████████████████████████████████████████████▍                                                                        | 9/20 [00:10<00:11,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'reference_contexts', 'gt_answer', 'test', 'query_metadata', 'inference_cost'])\n",
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate([]) == []\n",
      "    assert candidate([1, 2, 3, 4]) == [1, 2, 3, 4]\n",
      "    assert candidate([4, 3, 2, 1]) == [4, 4, 4, 4]\n",
      "    assert candidate([3, 2, 3, 100, 3]) == [3, 3, 3, 100, 100]\n",
      "\n",
      "cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|█████████████████████████████████████████████████████████████████▌                                                                 | 10/20 [00:11<00:10,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'reference_contexts', 'gt_answer', 'test', 'query_metadata', 'inference_cost'])\n",
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate('') == ''\n",
      "    assert candidate('x') == 'x'\n",
      "    assert candidate('xyz') == 'xyzyx'\n",
      "    assert candidate('xyx') == 'xyx'\n",
      "    assert candidate('jerry') == 'jerryrrej'\n",
      "\n",
      "cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  55%|████████████████████████████████████████████████████████████████████████                                                           | 11/20 [00:12<00:09,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'reference_contexts', 'gt_answer', 'test', 'query_metadata', 'inference_cost'])\n",
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate('111000', '101010') == '010010'\n",
      "    assert candidate('1', '1') == '0'\n",
      "    assert candidate('0101', '0000') == '0101'\n",
      "\n",
      "cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  60%|██████████████████████████████████████████████████████████████████████████████▌                                                    | 12/20 [00:13<00:08,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'reference_contexts', 'gt_answer', 'test', 'query_metadata', 'inference_cost'])\n",
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate([]) == None\n",
      "    assert candidate(['x', 'y', 'z']) == 'x'\n",
      "    assert candidate(['x', 'yyy', 'zzzz', 'www', 'kkkk', 'abc']) == 'zzzz'\n",
      "\n",
      "cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  65%|█████████████████████████████████████████████████████████████████████████████████████▏                                             | 13/20 [00:14<00:07,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'reference_contexts', 'gt_answer', 'test', 'query_metadata', 'inference_cost'])\n",
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate(3, 7) == 1\n",
      "    assert candidate(10, 15) == 5\n",
      "    assert candidate(49, 14) == 7\n",
      "    assert candidate(144, 60) == 12\n",
      "\n",
      "cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  70%|███████████████████████████████████████████████████████████████████████████████████████████▋                                       | 14/20 [00:15<00:06,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'reference_contexts', 'gt_answer', 'test', 'query_metadata', 'inference_cost'])\n",
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate('') == []\n",
      "    assert candidate('asdfgh') == ['a', 'as', 'asd', 'asdf', 'asdfg', 'asdfgh']\n",
      "    assert candidate('WWW') == ['W', 'WW', 'WWW']\n",
      "\n",
      "cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  75%|██████████████████████████████████████████████████████████████████████████████████████████████████▎                                | 15/20 [00:16<00:04,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'reference_contexts', 'gt_answer', 'test', 'query_metadata', 'inference_cost'])\n",
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate(0) == '0'\n",
      "    assert candidate(3) == '0 1 2 3'\n",
      "    assert candidate(10) == '0 1 2 3 4 5 6 7 8 9 10'\n",
      "\n",
      "cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  80%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊                          | 16/20 [00:17<00:03,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'reference_contexts', 'gt_answer', 'test', 'query_metadata', 'inference_cost'])\n",
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate('') == 0\n",
      "    assert candidate('abcde') == 5\n",
      "    assert candidate('abcde' + 'cade' + 'CADE') == 5\n",
      "    assert candidate('aaaaAAAAaaaa') == 1\n",
      "    assert candidate('Jerry jERRY JeRRRY') == 5\n",
      "\n",
      "cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 17/20 [00:18<00:02,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'reference_contexts', 'gt_answer', 'test', 'query_metadata', 'inference_cost'])\n",
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate('') == []\n",
      "    assert candidate('o o o o') == [4, 4, 4, 4]\n",
      "    assert candidate('.| .| .| .|') == [1, 1, 1, 1]\n",
      "    assert candidate('o| o| .| .| o o o o') == [2, 2, 1, 1, 4, 4, 4, 4]\n",
      "    assert candidate('o| .| o| .| o o| o o|') == [2, 1, 2, 1, 4, 2, 4, 2]\n",
      "\n",
      "cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉             | 18/20 [00:21<00:03,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'reference_contexts', 'gt_answer', 'test', 'query_metadata', 'inference_cost'])\n",
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate('', 'x') == 0\n",
      "    assert candidate('xyxyxyx', 'x') == 4\n",
      "    assert candidate('cacacacac', 'cac') == 4\n",
      "    assert candidate('john doe', 'john') == 1\n",
      "\n",
      "cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 19/20 [00:22<00:01,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'answer', 'guardrails_output_assessment', 'guardrails_context_assessment', 'guardrails_input_assessment', 'guardrails_blocked', 'guardrails_block_level', 'answer_metadata', 'reference_contexts', 'gt_answer', 'test', 'query_metadata', 'inference_cost'])\n",
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate('') == ''\n",
      "    assert candidate('three') == 'three'\n",
      "    assert candidate('three five nine') == 'three five nine'\n",
      "    assert candidate('five zero four seven nine eight') == 'zero four five seven eight nine'\n",
      "    assert candidate('six five four three two one zero') == 'zero one two three four five six'\n",
      "\n",
      "cohere.command-r-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:23<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "processor = CustomEvaluator(evaluator_llm_info = exp_config_data)\n",
    "results = processor.evaluate(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab2687e-1ea0-4f53-a09d-2e27b56d1290",
   "metadata": {},
   "source": [
    "## Save results to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feb01294-86c6-4f9d-9744-57c3ab68dd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = './results/evaluation_output.csv'\n",
    "\n",
    "# Check if 'sagemaker_cost' exists in any item\n",
    "include_sagemaker_cost = any('sagemaker_cost' in item for item in results)\n",
    "include_inference_cost = any('inference_cost' in item for item in results)\n",
    "\n",
    "fieldnames=['question', 'answer', 'inputTokens', 'outputTokens', 'totalTokens', 'latencyMs', 'ground answer','message','score']\n",
    "\n",
    "if include_sagemaker_cost:\n",
    "    fieldnames.insert(fieldnames.index('message'), 'sagemaker_cost')  # Insert before 'ground answer'\n",
    "\n",
    "if include_inference_cost:\n",
    "    fieldnames.insert(fieldnames.index('message'), 'bedrock_input_cost')  # Insert before 'ground answer'\n",
    "    fieldnames.insert(fieldnames.index('message'), 'bedrock_output_cost')  # Insert before 'ground answer'\n",
    "    \n",
    "\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for _id, item in enumerate(results):\n",
    "        answer_metadata = item.get('answer_metadata', {})\n",
    "        response = item.get('response', {})\n",
    "\n",
    "        row = {\n",
    "            'question': item.get('question', ''),\n",
    "            'answer': item.get('answer', ''),\n",
    "            'inputTokens': answer_metadata.get('inputTokens', ''),\n",
    "            'outputTokens': answer_metadata.get('outputTokens', ''),\n",
    "            'totalTokens': answer_metadata.get('totalTokens', ''),\n",
    "            'latencyMs': answer_metadata.get('latencyMs', ''),\n",
    "            'ground answer': item.get('gt_answer', ''),\n",
    "            'message': response.get('message', ''),\n",
    "            'score': response.get('score', ''),\n",
    "        }\n",
    "\n",
    "        if include_sagemaker_cost:\n",
    "            sagemaker_cost = item.get('sagemaker_cost', {})\n",
    "            row['sagemaker_cost'] = sagemaker_cost.get('sagemaker_cost', '')\n",
    "        if include_inference_cost:\n",
    "            inference_cost = item.get('inference_cost', {})\n",
    "            row['bedrock_input_cost'] = inference_cost.get('inference_input_cost', '')\n",
    "            row['bedrock_output_cost'] = inference_cost.get('inference_output_cost', '')\n",
    "\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f43b757-20be-4efc-b0f5-4de00d97ed67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
