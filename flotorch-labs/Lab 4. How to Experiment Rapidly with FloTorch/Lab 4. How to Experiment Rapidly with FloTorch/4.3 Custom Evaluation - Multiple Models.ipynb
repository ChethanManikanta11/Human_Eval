{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Multiple Models\n",
    "\n",
    "[FloTorch](https://www.flotorch.ai/) offers a robust evaluation framework for Retrieval-Augmented Generation (RAG) systems, enabling comprehensive assessment and comparison of Large Language Models (LLMs). It focuses on key metrics such as accuracy, cost, and latency, crucial for enterprise-level deployments.\n",
    "\n",
    "## Key Evaluation Metrics for this Notebook\n",
    "\n",
    "In this notebook, we will focus on evaluating our RAG pipelines using the following metrics:\n",
    "\n",
    "* **Correctness:** This refers to the total number of samples that semantically both generated and expected are mateched\n",
    "\n",
    "* **Inference Cost:** This refers to the total cost incurred for invoking Bedrock models to generate responses for all entries in the ground truth dataset.\n",
    "\n",
    "* **Latency:** This measures the time taken for the inference process, specifically the duration of the Bedrock model invocations.\n",
    "\n",
    "\n",
    "RAG systems are evaluated using a scoring method that measures response quality to questions in the evaluation set. Responses are rated as correct, Missing or incorrect:\n",
    "\n",
    "- correct: The response correctly answers the user question and contains no hallucinated content.\n",
    "\n",
    "- Missing: The answer does not provide the requested information. Such as “I don’t know”, “I’m sorry I can’t find …” or similar sentences without providing a concrete answer to the question.\n",
    "\n",
    "- Incorrect: The response provides wrong or irrelevant information to answer the user question\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accountNumber': '677276078734',\n",
       " 'regionName': 'us-east-1',\n",
       " 'bedrockExecutionRoleArn': 'arn:aws:iam::677276078734:role/advanced-rag-workshop-bedrock_execution_role-us-east-1',\n",
       " 's3Bucket': 'flotorch-benchmarking',\n",
       " 's3_ground_truth_path': 's3://flotorch-benchmarking/ground_truth_data/ground_truth.json'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_config_data = {\n",
    "   \"eval_embedding_model\" : \"amazon.titan-embed-text-v2:0\",\n",
    "   \"eval_retrieval_model\" : \"us.amazon.nova-pro-v1:0\",\n",
    "   \"eval_retrieval_service\" : \"bedrock\",\n",
    "   \"aws_region\" : variables['regionName'],\n",
    "   \"eval_embed_vector_dimension\" : 1024\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RAG response data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = f\"./results/ragas_evaluation_responses_for_different_models.json\"\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    loaded_responses = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Evaluation with Custom Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:37<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed for us.amazon.nova-lite-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:34<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed for us.amazon.nova-micro-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:31<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed for us.anthropic.claude-3-5-haiku-20241022-v1:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:28<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed for us.anthropic.claude-3-5-sonnet-20241022-v2:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from custom_evaluation import CustomEvaluator\n",
    "\n",
    "evaluator = CustomEvaluator(evaluator_llm_info = evaluation_config_data)\n",
    "evaluation_metrics = {}\n",
    "for model_id, inference_data in loaded_responses.items():\n",
    "    results = evaluator.evaluate(inference_data)\n",
    "    evaluation_metrics[model_id] = results\n",
    "    print(f\"Evaluation completed for {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_evaluation = evaluator.evaluate_results_dict(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost and Latency Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inference_cost': 0.00074322, 'average_inference_cost': 3.7161e-05, 'latency': 8270.0, 'average_latency': 413.5, 'processed_items': 20}\n",
      "{'inference_cost': 0.00035042, 'average_inference_cost': 1.7521e-05, 'latency': 6500.0, 'average_latency': 325.0, 'processed_items': 20}\n",
      "{'inference_cost': 0.0045628, 'average_inference_cost': 0.00022814, 'latency': 30611.0, 'average_latency': 1530.55, 'processed_items': 20}\n",
      "{'inference_cost': 0.036936, 'average_inference_cost': 0.0018468, 'latency': 31915.0, 'average_latency': 1595.75, 'processed_items': 20}\n"
     ]
    }
   ],
   "source": [
    "from cost_compute_utils import calculate_cost_and_latency_metrics\n",
    "\n",
    "for model in loaded_responses:\n",
    "    inference_data = loaded_responses[model]\n",
    "    cost_and_latency_metrics = calculate_cost_and_latency_metrics(inference_data, model,\n",
    "                evaluation_config_data[\"aws_region\"])\n",
    "    print(cost_and_latency_metrics)\n",
    "    if model not in final_evaluation:\n",
    "        # Insert - key doesn't exist yet\n",
    "        final_evaluation[model] = cost_and_latency_metrics\n",
    "    else:\n",
    "        # Update - key already exists\n",
    "        final_evaluation[model].update(cost_and_latency_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics as pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>number of samples correct</th>\n",
       "      <th>inference_cost</th>\n",
       "      <th>average_inference_cost</th>\n",
       "      <th>latency</th>\n",
       "      <th>average_latency</th>\n",
       "      <th>processed_items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>us.amazon.nova-lite-v1:0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>8270.0</td>\n",
       "      <td>413.50</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>us.amazon.nova-micro-v1:0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>325.00</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>us.anthropic.claude-3-5-haiku-20241022-v1:0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.004563</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>30611.0</td>\n",
       "      <td>1530.55</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>us.anthropic.claude-3-5-sonnet-20241022-v2:0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.036936</td>\n",
       "      <td>0.001847</td>\n",
       "      <td>31915.0</td>\n",
       "      <td>1595.75</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          model  number of samples correct  \\\n",
       "0                      us.amazon.nova-lite-v1:0                         20   \n",
       "1                     us.amazon.nova-micro-v1:0                         16   \n",
       "2   us.anthropic.claude-3-5-haiku-20241022-v1:0                         19   \n",
       "3  us.anthropic.claude-3-5-sonnet-20241022-v2:0                         20   \n",
       "\n",
       "   inference_cost  average_inference_cost  latency  average_latency  \\\n",
       "0        0.000743                0.000037   8270.0           413.50   \n",
       "1        0.000350                0.000018   6500.0           325.00   \n",
       "2        0.004563                0.000228  30611.0          1530.55   \n",
       "3        0.036936                0.001847  31915.0          1595.75   \n",
       "\n",
       "   processed_items  \n",
       "0               20  \n",
       "1               20  \n",
       "2               20  \n",
       "3               20  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the nested dictionary to a DataFrame\n",
    "evaluation_df = pd.DataFrame.from_dict(final_evaluation, orient='index')\n",
    "\n",
    "# If you want the kb_type as a column instead of an index\n",
    "evaluation_df = evaluation_df.reset_index().rename(columns={'index': 'model'})\n",
    "\n",
    "evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
