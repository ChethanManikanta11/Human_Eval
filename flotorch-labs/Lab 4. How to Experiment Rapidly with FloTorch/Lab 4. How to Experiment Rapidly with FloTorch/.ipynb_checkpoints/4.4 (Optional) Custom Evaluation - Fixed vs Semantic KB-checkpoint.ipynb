{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating\n",
    "\n",
    "\n",
    "## Key Evaluation Metrics for this Notebook\n",
    "\n",
    "In this notebook, we will focus on evaluating our RAG pipelines using the following metrics:\n",
    "\n",
    "* **Correctness:** This refers to the total number of samples that semantically both generated and expected are mateched\n",
    "\n",
    "* **Inference Cost:** This refers to the total cost incurred for invoking Bedrock models to generate responses for all entries in the ground truth dataset.\n",
    "\n",
    "* **Latency:** This measures the time taken for the inference process, specifically the duration of the Bedrock model invocations.\n",
    "\n",
    "\n",
    "RAG systems are evaluated using a scoring method that measures response quality to questions in the evaluation set. Responses are rated as correct, Missing or incorrect:\n",
    "\n",
    "- correct: The response correctly answers the user question and contains no hallucinated content.\n",
    "\n",
    "- Missing: The answer does not provide the requested information. Such as “I don’t know”, “I’m sorry I can’t find …” or similar sentences without providing a concrete answer to the question.\n",
    "\n",
    "- Incorrect: The response provides wrong or irrelevant information to answer the user question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accountNumber': '677276078734',\n",
       " 'regionName': 'us-east-1',\n",
       " 'collectionArn': 'arn:aws:aoss:us-west-2:746074413210:collection/3f35uv3lze9bdothrm0c',\n",
       " 'collectionId': '3f35uv3lze9bdothrm0c',\n",
       " 'vectorIndexName': 'ws-index-',\n",
       " 'bedrockExecutionRoleArn': 'arn:aws:iam::677276078734:role/advanced-rag-workshop-bedrock_execution_role-us-east-1',\n",
       " 's3Bucket': 'flotorch-benchmarking',\n",
       " 'kbFixedChunk': 'WO4U6AWAU1',\n",
       " 'kbSemanticChunk': 'OUFEWBGEES',\n",
       " 'kbHierarchicalChunk': 'IHWIS6EP0H',\n",
       " 's3_ground_truth_path': 's3://flotorch-benchmarking/ground_truth_data/ground_truth.json'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Config\n",
    "\n",
    "We will evaluate the RAG pipeline using Amazon Nova Pro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_config_data = {\n",
    "   \"eval_embedding_model\" : \"amazon.titan-embed-text-v2:0\",\n",
    "   \"eval_retrieval_model\" : \"us.amazon.nova-pro-v1:0\",\n",
    "   \"eval_retrieval_service\" : \"bedrock\",\n",
    "   \"aws_region\" : variables['regionName'],\n",
    "   \"eval_embed_vector_dimension\" : 1024,\n",
    "   \"inference_model\": \"us.amazon.nova-lite-v1:0\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RAG response data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = f\"./results/ragas_evaluation_responses_for_different_kbs.json\"\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    loaded_responses = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy with Custom Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:31<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from custom_evaluation import CustomEvaluator\n",
    "\n",
    "evaluator = CustomEvaluator(evaluator_llm_info = evaluation_config_data)\n",
    "results = evaluator.evaluate(loaded_responses)\n",
    "print(f\"Evaluation completed\")\n",
    "\n",
    "final_evaluation = evaluator.evaluate_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'number of samples correct': 20}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost and Latency Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inference_cost': 0.00093516, 'average_inference_cost': 4.6757999999999995e-05, 'latency': 10828.0, 'average_latency': 541.4, 'processed_items': 20}\n"
     ]
    }
   ],
   "source": [
    "from cost_compute_utils import calculate_cost_and_latency_metrics\n",
    "\n",
    "inference_data = results\n",
    "cost_and_latency_metrics = calculate_cost_and_latency_metrics(inference_data, evaluation_config_data[\"inference_model\"],\n",
    "            evaluation_config_data[\"aws_region\"])\n",
    "\n",
    "print(cost_and_latency_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
